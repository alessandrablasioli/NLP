{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# pytorch lightning\n",
    "from lightning import LightningModule\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "\n",
    "import pandas as pd\n",
    "import utils"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.760671300Z",
     "start_time": "2023-12-10T14:37:36.708460500Z"
    }
   },
   "id": "7b3cd2c7bd257608"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.815797Z",
     "start_time": "2023-12-10T14:37:36.763285800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  Argument ID  Self-direction: thought  Self-direction: action  Stimulation  \\\n0      A26004                        0                       0            0   \n1      A26010                        0                       0            0   \n2      A26016                        0                       0            0   \n3      A26024                        0                       0            0   \n4      A26026                        0                       0            0   \n\n   Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\n0         0            1                 0                 0     0   \n1         0            1                 0                 0     0   \n2         0            1                 0                 0     0   \n3         0            1                 0                 0     0   \n4         0            1                 0                 0     0   \n\n   Security: personal  ...  Tradition  Conformity: rules  \\\n0                   1  ...          0                  0   \n1                   0  ...          0                  0   \n2                   1  ...          0                  0   \n3                   0  ...          0                  0   \n4                   1  ...          0                  0   \n\n   Conformity: interpersonal  Humility  Benevolence: caring  \\\n0                          0         0                    0   \n1                          0         0                    0   \n2                          0         0                    0   \n3                          0         0                    0   \n4                          0         0                    1   \n\n   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n0                           0                      1                     0   \n1                           0                      1                     0   \n2                           1                      1                     0   \n3                           0                      0                     0   \n4                           1                      0                     0   \n\n   Universalism: tolerance  Universalism: objectivity  \n0                        1                          0  \n1                        1                          1  \n2                        0                          0  \n3                        0                          0  \n4                        0                          0  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Argument ID</th>\n      <th>Self-direction: thought</th>\n      <th>Self-direction: action</th>\n      <th>Stimulation</th>\n      <th>Hedonism</th>\n      <th>Achievement</th>\n      <th>Power: dominance</th>\n      <th>Power: resources</th>\n      <th>Face</th>\n      <th>Security: personal</th>\n      <th>...</th>\n      <th>Tradition</th>\n      <th>Conformity: rules</th>\n      <th>Conformity: interpersonal</th>\n      <th>Humility</th>\n      <th>Benevolence: caring</th>\n      <th>Benevolence: dependability</th>\n      <th>Universalism: concern</th>\n      <th>Universalism: nature</th>\n      <th>Universalism: tolerance</th>\n      <th>Universalism: objectivity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A26004</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A26010</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A26016</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A26024</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A26026</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_arg_train = pd.read_csv('./data/arguments-training.tsv', sep='\\t')\n",
    "df_arg_test = pd.read_csv('./data/arguments-test.tsv', sep='\\t')\n",
    "df_arg_val = pd.read_csv('./data/arguments-validation.tsv', sep='\\t')\n",
    "\n",
    "df_labels_train = pd.read_csv('./data/labels-training.tsv', sep='\\t')\n",
    "df_labels_test = pd.read_csv('./data/labels-test.tsv', sep='\\t')\n",
    "df_labels_val = pd.read_csv('./data/labels-validation.tsv', sep='\\t')\n",
    "\n",
    "df_labels_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "  Argument ID  Openness to change  Self-enhancement  Conservation  \\\n0      A26004                   0                 1             1   \n1      A26010                   0                 1             0   \n2      A26016                   0                 1             1   \n3      A26024                   0                 1             0   \n4      A26026                   0                 1             1   \n\n   Self-transcendence  \n0                   1  \n1                   1  \n2                   1  \n3                   0  \n4                   1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Argument ID</th>\n      <th>Openness to change</th>\n      <th>Self-enhancement</th>\n      <th>Conservation</th>\n      <th>Self-transcendence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A26004</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A26010</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A26016</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A26024</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A26026</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_3_categories = [\"Openness to change\", \"Self-enhancement\", \"Conservation\", \"Self-transcendence\"]\n",
    "\n",
    "level_3_to_2_mapping = {\n",
    "    \"Openness to change\": [\n",
    "        \"Self-direction: thought\",\n",
    "        \"Self-direction: action\",\n",
    "        \"Stimulation\",\n",
    "        \"Hedonism\",\n",
    "    ],\n",
    "    \"Self-enhancement\": [\n",
    "        \"Hedonism\",\n",
    "        \"Achievement\",\n",
    "        \"Power: dominance\",\n",
    "        \"Power: resources\",\n",
    "        \"Face\",\n",
    "    ],\n",
    "    \"Conservation\": [\n",
    "        \"Security: personal\",\n",
    "        \"Security: societal\",\n",
    "        \"Conformity: rules\",\n",
    "        \"Conformity: interpersonal\",\n",
    "        \"Tradition\",\n",
    "        \"Face\",\n",
    "        \"Humility\",\n",
    "    ],\n",
    "    \"Self-transcendence\": [\n",
    "        \"Benevolence: caring\",\n",
    "        \"Benevolence: dependability\",\n",
    "        \"Universalism: concern\",\n",
    "        \"Universalism: nature\",\n",
    "        \"Universalism: tolerance\",\n",
    "        \"Universalism: objectivity\",\n",
    "        \"Humility\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "column_to_drop = [x for l in level_3_to_2_mapping.values() for x in l]\n",
    "\n",
    "for category in level_3_categories:\n",
    "    # make a logical OR of all the level 2 categories\n",
    "    df_labels_test[category] = df_labels_test[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "    df_labels_val[category] = df_labels_val[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "    df_labels_train[category] = df_labels_train[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "\n",
    "df_labels_test = df_labels_test.drop(columns=column_to_drop)\n",
    "df_labels_val = df_labels_val.drop(columns=column_to_drop)\n",
    "df_labels_train = df_labels_train.drop(columns=column_to_drop)\n",
    "\n",
    "df_labels_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.855656Z",
     "start_time": "2023-12-10T14:37:36.815797Z"
    }
   },
   "id": "e2b4fe2c6cb107d4"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "  Argument ID                                   Conclusion       Stance  \\\n0      A01002                  We should ban human cloning  in favor of   \n1      A01005                      We should ban fast food  in favor of   \n2      A01006  We should end the use of economic sanctions      against   \n3      A01007         We should abolish capital punishment      against   \n4      A01008                We should ban factory farming      against   \n\n                                             Premise  Openness to change  \\\n0  we should ban human cloning as it will only ca...                   0   \n1  fast food should be banned because it is reall...                   0   \n2  sometimes economic sanctions are the only thin...                   0   \n3  capital punishment is sometimes the only optio...                   0   \n4  factory farming allows for the production of c...                   0   \n\n   Self-enhancement  Conservation  Self-transcendence  \n0                 0             1                   0  \n1                 0             1                   0  \n2                 1             1                   0  \n3                 0             1                   1  \n4                 0             1                   1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Argument ID</th>\n      <th>Conclusion</th>\n      <th>Stance</th>\n      <th>Premise</th>\n      <th>Openness to change</th>\n      <th>Self-enhancement</th>\n      <th>Conservation</th>\n      <th>Self-transcendence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A01002</td>\n      <td>We should ban human cloning</td>\n      <td>in favor of</td>\n      <td>we should ban human cloning as it will only ca...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A01005</td>\n      <td>We should ban fast food</td>\n      <td>in favor of</td>\n      <td>fast food should be banned because it is reall...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A01006</td>\n      <td>We should end the use of economic sanctions</td>\n      <td>against</td>\n      <td>sometimes economic sanctions are the only thin...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A01007</td>\n      <td>We should abolish capital punishment</td>\n      <td>against</td>\n      <td>capital punishment is sometimes the only optio...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A01008</td>\n      <td>We should ban factory farming</td>\n      <td>against</td>\n      <td>factory farming allows for the production of c...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.merge(df_arg_train, df_labels_train, on='Argument ID')\n",
    "df_test = pd.merge(df_arg_test, df_labels_test, on='Argument ID')\n",
    "df_val = pd.merge(df_arg_val, df_labels_val, on='Argument ID')\n",
    "\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.907343500Z",
     "start_time": "2023-12-10T14:37:36.842672700Z"
    }
   },
   "id": "8316839c9de44b89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1.5 Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72cdfe3378972543"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "  Argument ID                                   Conclusion  Stance  \\\n0      A01002                  We should ban human cloning       1   \n1      A01005                      We should ban fast food       1   \n2      A01006  We should end the use of economic sanctions       0   \n3      A01007         We should abolish capital punishment       0   \n4      A01008                We should ban factory farming       0   \n\n                                             Premise  Openness to change  \\\n0  we should ban human cloning as it will only ca...                   0   \n1  fast food should be banned because it is reall...                   0   \n2  sometimes economic sanctions are the only thin...                   0   \n3  capital punishment is sometimes the only optio...                   0   \n4  factory farming allows for the production of c...                   0   \n\n   Self-enhancement  Conservation  Self-transcendence  \n0                 0             1                   0  \n1                 0             1                   0  \n2                 1             1                   0  \n3                 0             1                   1  \n4                 0             1                   1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Argument ID</th>\n      <th>Conclusion</th>\n      <th>Stance</th>\n      <th>Premise</th>\n      <th>Openness to change</th>\n      <th>Self-enhancement</th>\n      <th>Conservation</th>\n      <th>Self-transcendence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A01002</td>\n      <td>We should ban human cloning</td>\n      <td>1</td>\n      <td>we should ban human cloning as it will only ca...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A01005</td>\n      <td>We should ban fast food</td>\n      <td>1</td>\n      <td>fast food should be banned because it is reall...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A01006</td>\n      <td>We should end the use of economic sanctions</td>\n      <td>0</td>\n      <td>sometimes economic sanctions are the only thin...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A01007</td>\n      <td>We should abolish capital punishment</td>\n      <td>0</td>\n      <td>capital punishment is sometimes the only optio...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A01008</td>\n      <td>We should ban factory farming</td>\n      <td>0</td>\n      <td>factory farming allows for the production of c...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode stance into 0, 1 \n",
    "\n",
    "df_train[\"Stance\"] = df_train[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "df_test[\"Stance\"] = df_test[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "df_val[\"Stance\"] = df_val[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.931489Z",
     "start_time": "2023-12-10T14:37:36.859755100Z"
    }
   },
   "id": "232f6776a1ee9a80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77076cfdcc028fa7"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "class ArgumentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            \"Premise\": row[\"Premise\"],\n",
    "            \"Conclusion\": row[\"Conclusion\"],\n",
    "            \"labels\": torch.tensor(row[level_3_categories].values.tolist(), dtype=torch.float32),\n",
    "            \"Stance\": torch.tensor(row[\"Stance\"], dtype=torch.float32)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.932511700Z",
     "start_time": "2023-12-10T14:37:36.873795800Z"
    }
   },
   "id": "ecad75b6100c0040"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Premise': 'we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same.', 'Conclusion': 'We should ban human cloning', 'labels': tensor([0., 0., 1., 0.]), 'Stance': tensor(1.)}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgumentDataset(df_train)\n",
    "test_dataset = ArgumentDataset(df_test)\n",
    "val_dataset = ArgumentDataset(df_val)\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_dataset[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.933557300Z",
     "start_time": "2023-12-10T14:37:36.890425400Z"
    }
   },
   "id": "94fe31d775254160"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3 Metric definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619bebf9aba984a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2 Model definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8ffa5c2876fecbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random and Majority Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffbd8b41505ded6"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "class RandomUniformClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        self._random_state = np.random.RandomState()\n",
    "\n",
    "    def predict(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        logits = self._random_state.uniform(size=(batch_size, 4))\n",
    "        logits = logits > 0.5\n",
    "        return torch.tensor(logits, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class MajorityClassifier(LightningModule):\n",
    "    def __init__(self, n_random_classifiers=10):\n",
    "        self.n_random_classifiers = n_random_classifiers\n",
    "        self.random_classifiers = [RandomUniformClassifier() for _ in range(n_random_classifiers)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        votes = torch.zeros((batch_size, 4))\n",
    "        for clf in self.random_classifiers:\n",
    "            votes += clf.predict(X)\n",
    "        votes = votes / self.n_random_classifiers\n",
    "        votes = votes > 0.5\n",
    "        return torch.tensor(votes, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.992878200Z",
     "start_time": "2023-12-10T14:37:36.908372600Z"
    }
   },
   "id": "331380fdc13c462f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bert models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab6e80eded57efb"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class BertConclusion(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded):\n",
    "        outputs = self.bert(**encoded)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"test_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"test_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:36.994917600Z",
     "start_time": "2023-12-10T14:37:36.926887Z"
    }
   },
   "id": "eef0d525e8b3e87e"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "class BertPremiseConclusion(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded_1, encoded_2):\n",
    "        output_1 = self.bert(**encoded_1)\n",
    "        output_2 = self.bert(**encoded_2)\n",
    "\n",
    "        output = torch.cat((output_1.last_hidden_state[:, 0, :], output_2.last_hidden_state[:, 0, :]), dim=1)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"test_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"test_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:37.058922700Z",
     "start_time": "2023-12-10T14:37:36.943501900Z"
    }
   },
   "id": "31dbbe5fe8eb3195"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "class BertPremiseConclusionStance(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2 + 1, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded_1, encoded_2, stance):\n",
    "        output_1 = self.bert(**encoded_1).last_hidden_state[:, 0, :]\n",
    "        output_2 = self.bert(**encoded_2).last_hidden_state[:, 0, :]\n",
    "        stance = stance.unsqueeze(1)\n",
    "        output = torch.cat((output_1, output_2, stance), dim=1)\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, stance = data[\"Premise\"], data[\"Conclusion\"], data[\"Stance\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, stance)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, stance = data[\"Premise\"], data[\"Conclusion\"], data[\"Stance\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, stance)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, stance = data[\"Premise\"], data[\"Conclusion\"], data[\"Stance\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, stance)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"test_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"test_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:37.079773700Z",
     "start_time": "2023-12-10T14:37:36.960565700Z"
    }
   },
   "id": "b11eb0f4965923dd"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "# Fix all possible sources of randomness\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:37.093619100Z",
     "start_time": "2023-12-10T14:37:36.970437700Z"
    }
   },
   "id": "e4d886bff0ede4b5"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping training...\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "logs_path = Path.cwd() / \"logs\" / \"lightning_logs\"\n",
    "train = False\n",
    "\n",
    "seeds = [6, 90, 157]\n",
    "\n",
    "epochs = 1\n",
    "output_dim = len(level_3_categories)  # +1 for padding\n",
    "\n",
    "model_classes = [BertConclusion, BertPremiseConclusion, BertPremiseConclusionStance]\n",
    "model_names = [\"bert_w_c\", \"bert_w_cp\", \"bert_w_cps\"]\n",
    "hyperparameters = [\n",
    "    {'bert_model_name': 'bert-base-uncased', 'num_classes': output_dim},\n",
    "    {'bert_model_name': 'bert-base-uncased', 'num_classes': output_dim},\n",
    "    {'bert_model_name': 'bert-base-uncased', 'num_classes': output_dim}\n",
    "]\n",
    "\n",
    "if train:\n",
    "    for model_class, model_name, hyperparameter in zip(model_classes, model_names, hyperparameters):\n",
    "        for seed in seeds:\n",
    "            print(f\"Training model {model_name} with seed {seed}...\")\n",
    "            seed_everything(seed, workers=True)\n",
    "\n",
    "            model = model_class(**hyperparameter)\n",
    "\n",
    "            logger = TensorBoardLogger(logs_path, name=f\"{model_name}_seed{seed}\")\n",
    "            checkpoint_callback = ModelCheckpoint(\n",
    "                monitor='val_loss',\n",
    "                dirpath=None,\n",
    "                filename=f'{model_name}-seed={seed}' + '-{epoch:02d}-{val_loss:.2f}-{val_f1:.2f}',\n",
    "                save_top_k=1,\n",
    "            )\n",
    "            early_stop_callback = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                verbose=True,\n",
    "                mode='min'\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                max_epochs=epochs,\n",
    "                logger=logger,\n",
    "                log_every_n_steps=1,\n",
    "                callbacks=[checkpoint_callback, early_stop_callback],\n",
    "                deterministic=True\n",
    "            )\n",
    "\n",
    "            trainer.fit(model, train_dataloader, val_dataloader)\n",
    "else:\n",
    "    print(\"Skipping training...\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T14:37:37.109702500Z",
     "start_time": "2023-12-10T14:37:36.988185200Z"
    }
   },
   "id": "499c93fc7632db8e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, model_type):\n",
    "    if model_type == \"bert_w_c\":\n",
    "        prediction = utils.model_bert_c_predict(model, loader)\n",
    "    elif model_type == \"bert_w_cp\":\n",
    "        prediction = utils.model_bert_cp_predict(model, loader)\n",
    "    elif model_type == \"bert_w_cps\":\n",
    "        prediction = utils.model_bert_cps_predict(model, loader)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type\")\n",
    "\n",
    "    f1_metric = MultilabelF1Score(num_labels=4, average=None, multidim_average='global')\n",
    "    \n",
    "    #Take the target from the loader\n",
    "    target = torch.cat([data[\"labels\"] for data in loader], dim=0)\n",
    "\n",
    "    results = f1_metric(prediction, target)\n",
    "    average = sum(results) / 4\n",
    "\n",
    "    print(\"F1 Score for each class:\")\n",
    "    for i, category in enumerate(level_3_categories):\n",
    "        print(f\"{category}: {results[i]:.4f}\")\n",
    "\n",
    "    print(f\"Overall F1 Score: {average:.4f}\")\n",
    "\n",
    "    return average\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c0839fab615304c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
