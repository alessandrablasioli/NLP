{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# pytorch lightning\n",
    "from lightning import LightningModule\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b3cd2c7bd257608"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_arg_train = pd.read_csv('./data/arguments-training.tsv', sep='\\t')\n",
    "df_arg_test = pd.read_csv('./data/arguments-test.tsv', sep='\\t')\n",
    "df_arg_val = pd.read_csv('./data/arguments-validation.tsv', sep='\\t')\n",
    "\n",
    "df_labels_train = pd.read_csv('./data/labels-training.tsv', sep='\\t')\n",
    "df_labels_test = pd.read_csv('./data/labels-test.tsv', sep='\\t')\n",
    "df_labels_val = pd.read_csv('./data/labels-validation.tsv', sep='\\t')\n",
    "\n",
    "df_labels_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "level_3_categories = [\"Openness to change\", \"Self-enhancement\", \"Conservation\", \"Self-transcendence\"]\n",
    "\n",
    "level_3_to_2_mapping = {\n",
    "    \"Openness to change\": [\n",
    "        \"Self-direction: thought\",\n",
    "        \"Self-direction: action\",\n",
    "        \"Stimulation\",\n",
    "        \"Hedonism\",\n",
    "    ],\n",
    "    \"Self-enhancement\": [\n",
    "        \"Hedonism\",\n",
    "        \"Achievement\",\n",
    "        \"Power: dominance\",\n",
    "        \"Power: resources\",\n",
    "        \"Face\",\n",
    "    ],\n",
    "    \"Conservation\": [\n",
    "        \"Security: personal\",\n",
    "        \"Security: societal\",\n",
    "        \"Conformity: rules\",\n",
    "        \"Conformity: interpersonal\",\n",
    "        \"Tradition\",\n",
    "        \"Face\",\n",
    "        \"Humility\",\n",
    "    ],\n",
    "    \"Self-transcendence\": [\n",
    "        \"Benevolence: caring\",\n",
    "        \"Benevolence: dependability\",\n",
    "        \"Universalism: concern\",\n",
    "        \"Universalism: nature\",\n",
    "        \"Universalism: tolerance\",\n",
    "        \"Universalism: objectivity\",\n",
    "        \"Humility\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "column_to_drop = [x for l in level_3_to_2_mapping.values() for x in l]\n",
    "\n",
    "for category in level_3_categories:\n",
    "    # make a logical OR of all the level 2 categories\n",
    "    df_labels_test[category] = df_labels_test[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "    df_labels_val[category] = df_labels_val[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "    df_labels_train[category] = df_labels_train[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "\n",
    "df_labels_test = df_labels_test.drop(columns=column_to_drop)\n",
    "df_labels_val = df_labels_val.drop(columns=column_to_drop)\n",
    "df_labels_train = df_labels_train.drop(columns=column_to_drop)\n",
    "\n",
    "df_labels_test.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2b4fe2c6cb107d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_arg_train, df_labels_train, on='Argument ID')\n",
    "df_test = pd.merge(df_arg_test, df_labels_test, on='Argument ID')\n",
    "df_val = pd.merge(df_arg_val, df_labels_val, on='Argument ID')\n",
    "\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8316839c9de44b89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1.5 Encoding"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72cdfe3378972543"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Encode stance into 0, 1 \n",
    "\n",
    "df_train[\"Stance\"] = df_train[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "df_test[\"Stance\"] = df_test[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "df_val[\"Stance\"] = df_val[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "\n",
    "df_train.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "232f6776a1ee9a80"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77076cfdcc028fa7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ArgumentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            \"Premise\": row[\"Premise\"],\n",
    "            \"Conclusion\": row[\"Conclusion\"],\n",
    "            \"labels\": torch.tensor(row[level_3_categories].values.tolist(), dtype=torch.float32),\n",
    "            \"Stance\": torch.tensor(row[\"Stance\"], dtype=torch.float32)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecad75b6100c0040"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = ArgumentDataset(df_train)\n",
    "test_dataset = ArgumentDataset(df_test)\n",
    "val_dataset = ArgumentDataset(df_val)\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_dataset[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94fe31d775254160"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 3 Metric definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "619bebf9aba984a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 2 Model definition"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8ffa5c2876fecbf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random and Majority Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffbd8b41505ded6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RandomUniformClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        self._random_state = np.random.RandomState()\n",
    "\n",
    "    def predict(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        logits = self._random_state.uniform(size=(batch_size, 4))\n",
    "        logits = logits > 0.5\n",
    "        return torch.tensor(logits, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class MajorityClassifier(LightningModule):\n",
    "    def __init__(self, n_random_classifiers=10):\n",
    "        self.n_random_classifiers = n_random_classifiers\n",
    "        self.random_classifiers = [RandomUniformClassifier() for _ in range(n_random_classifiers)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        votes = torch.zeros((batch_size, 4))\n",
    "        for clf in self.random_classifiers:\n",
    "            votes += clf.predict(X)\n",
    "        votes = votes / self.n_random_classifiers\n",
    "        votes = votes > 0.5\n",
    "        return torch.tensor(votes, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "331380fdc13c462f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bert models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab6e80eded57efb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class BertConclusion(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded):\n",
    "        outputs = self.bert(**encoded)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"test_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"test_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eef0d525e8b3e87e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BertPremiseConclusion(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded_1, encoded_2):\n",
    "        output_1 = self.bert(**encoded_1)\n",
    "        output_2 = self.bert(**encoded_2)\n",
    "\n",
    "        output = torch.cat((output_1.last_hidden_state[:, 0, :], output_2.last_hidden_state[:, 0, :]), dim=1)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"test_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"test_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31dbbe5fe8eb3195"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BertPremiseConclusionStance(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2 + 1, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded_1, encoded_2, stance):\n",
    "        output_1 = self.bert(**encoded_1).last_hidden_state[:, 0, :]\n",
    "        output_2 = self.bert(**encoded_2).last_hidden_state[:, 0, :]\n",
    "        stance = stance.unsqueeze(1)\n",
    "        output = torch.cat((output_1, output_2, stance), dim=1)\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, stance = data[\"Premise\"], data[\"Conclusion\"], data[\"Stance\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, stance)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, stance = data[\"Premise\"], data[\"Conclusion\"], data[\"Stance\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, stance)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, stance = data[\"Premise\"], data[\"Conclusion\"], data[\"Stance\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, stance)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"test_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"test_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b11eb0f4965923dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=TensorBoardLogger(\"lightning_logs\", name=\"bert\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val_loss\"),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3),\n",
    "    ],\n",
    ")\n",
    "\n",
    "bertClassifier = BertPremiseConclusionStance(\"bert-base-uncased\", num_classes=len(level_3_categories))\n",
    "\n",
    "trainer.fit(bertClassifier, train_dataloader, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5ae8c52cc463208"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fix all possible sources of randomness\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4d886bff0ede4b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "logs_path = Path.cwd() / \"logs\" / \"lightning_logs\"\n",
    "train = True\n",
    "\n",
    "seeds = [6, 90, 157]\n",
    "\n",
    "epochs = 1\n",
    "output_dim = len(level_3_categories)  # +1 for padding\n",
    "\n",
    "model_classes = [BertConclusion, BertPremiseConclusion, BertPremiseConclusionStance]\n",
    "model_names = [\"bert_w_c\", \"bert_w_cp\", \"bert_w_cps\"]\n",
    "hyperparameters = [\n",
    "    {'bert_model_name': 'bert-base-uncased', 'num_classes': output_dim},\n",
    "    {'bert_model_name': 'bert-base-uncased', 'num_classes': output_dim},\n",
    "    {'bert_model_name': 'bert-base-uncased', 'num_classes': output_dim}\n",
    "]\n",
    "\n",
    "if train:\n",
    "    for model_class, model_name, hyperparameter in zip(model_classes, model_names, hyperparameters):\n",
    "        for seed in seeds:\n",
    "            print(f\"Training model {model_name} with seed {seed}...\")\n",
    "            seed_everything(seed, workers=True)\n",
    "\n",
    "            model = model_class(**hyperparameter)\n",
    "\n",
    "            logger = TensorBoardLogger(logs_path, name=f\"{model_name}_seed{seed}\")\n",
    "            checkpoint_callback = ModelCheckpoint(\n",
    "                monitor='val_loss',\n",
    "                dirpath=None,\n",
    "                filename=f'{model_name}-seed={seed}' + '-{epoch:02d}-{val_loss:.2f}-{val_f1:.2f}',\n",
    "                save_top_k=1,\n",
    "            )\n",
    "            early_stop_callback = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                verbose=True,\n",
    "                mode='min'\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                max_epochs=epochs,\n",
    "                logger=logger,\n",
    "                log_every_n_steps=1,\n",
    "                callbacks=[checkpoint_callback, early_stop_callback],\n",
    "                deterministic=True\n",
    "            )\n",
    "\n",
    "            trainer.fit(model, train_dataloader, val_dataloader)\n",
    "else:\n",
    "    print(\"Skipping training...\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "499c93fc7632db8e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
