{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b3cd2c7bd257608",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# pytorch lightning\n",
    "from lightning import LightningModule\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "\n",
    "import pandas as pd\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cabe9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 #choose a seed\n",
    "\n",
    "def fix_seed(seed: int) -> None:\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Self-direction: thought</th>\n",
       "      <th>Self-direction: action</th>\n",
       "      <th>Stimulation</th>\n",
       "      <th>Hedonism</th>\n",
       "      <th>Achievement</th>\n",
       "      <th>Power: dominance</th>\n",
       "      <th>Power: resources</th>\n",
       "      <th>Face</th>\n",
       "      <th>Security: personal</th>\n",
       "      <th>...</th>\n",
       "      <th>Tradition</th>\n",
       "      <th>Conformity: rules</th>\n",
       "      <th>Conformity: interpersonal</th>\n",
       "      <th>Humility</th>\n",
       "      <th>Benevolence: caring</th>\n",
       "      <th>Benevolence: dependability</th>\n",
       "      <th>Universalism: concern</th>\n",
       "      <th>Universalism: nature</th>\n",
       "      <th>Universalism: tolerance</th>\n",
       "      <th>Universalism: objectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A26004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A26010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A26016</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A26024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A26026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID  Self-direction: thought  Self-direction: action  Stimulation  \\\n",
       "0      A26004                        0                       0            0   \n",
       "1      A26010                        0                       0            0   \n",
       "2      A26016                        0                       0            0   \n",
       "3      A26024                        0                       0            0   \n",
       "4      A26026                        0                       0            0   \n",
       "\n",
       "   Hedonism  Achievement  Power: dominance  Power: resources  Face  \\\n",
       "0         0            1                 0                 0     0   \n",
       "1         0            1                 0                 0     0   \n",
       "2         0            1                 0                 0     0   \n",
       "3         0            1                 0                 0     0   \n",
       "4         0            1                 0                 0     0   \n",
       "\n",
       "   Security: personal  ...  Tradition  Conformity: rules  \\\n",
       "0                   1  ...          0                  0   \n",
       "1                   0  ...          0                  0   \n",
       "2                   1  ...          0                  0   \n",
       "3                   0  ...          0                  0   \n",
       "4                   1  ...          0                  0   \n",
       "\n",
       "   Conformity: interpersonal  Humility  Benevolence: caring  \\\n",
       "0                          0         0                    0   \n",
       "1                          0         0                    0   \n",
       "2                          0         0                    0   \n",
       "3                          0         0                    0   \n",
       "4                          0         0                    1   \n",
       "\n",
       "   Benevolence: dependability  Universalism: concern  Universalism: nature  \\\n",
       "0                           0                      1                     0   \n",
       "1                           0                      1                     0   \n",
       "2                           1                      1                     0   \n",
       "3                           0                      0                     0   \n",
       "4                           1                      0                     0   \n",
       "\n",
       "   Universalism: tolerance  Universalism: objectivity  \n",
       "0                        1                          0  \n",
       "1                        1                          1  \n",
       "2                        0                          0  \n",
       "3                        0                          0  \n",
       "4                        0                          0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_arg_train = pd.read_csv('./data/arguments-training.tsv', sep='\\t')\n",
    "df_arg_test = pd.read_csv('./data/arguments-test.tsv', sep='\\t')\n",
    "df_arg_val = pd.read_csv('./data/arguments-validation.tsv', sep='\\t')\n",
    "\n",
    "df_labels_train = pd.read_csv('./data/labels-training.tsv', sep='\\t')\n",
    "df_labels_test = pd.read_csv('./data/labels-test.tsv', sep='\\t')\n",
    "df_labels_val = pd.read_csv('./data/labels-validation.tsv', sep='\\t')\n",
    "\n",
    "df_labels_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2b4fe2c6cb107d4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Openness to change</th>\n",
       "      <th>Self-enhancement</th>\n",
       "      <th>Conservation</th>\n",
       "      <th>Self-transcendence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A26004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A26010</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A26016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A26024</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A26026</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID  Openness to change  Self-enhancement  Conservation  \\\n",
       "0      A26004                   0                 1             1   \n",
       "1      A26010                   0                 1             0   \n",
       "2      A26016                   0                 1             1   \n",
       "3      A26024                   0                 1             0   \n",
       "4      A26026                   0                 1             1   \n",
       "\n",
       "   Self-transcendence  \n",
       "0                   1  \n",
       "1                   1  \n",
       "2                   1  \n",
       "3                   0  \n",
       "4                   1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_3_categories = [\"Openness to change\", \"Self-enhancement\", \"Conservation\", \"Self-transcendence\"]\n",
    "\n",
    "level_3_to_2_mapping = {\n",
    "    \"Openness to change\": [\n",
    "        \"Self-direction: thought\",\n",
    "        \"Self-direction: action\",\n",
    "        \"Stimulation\",\n",
    "        \"Hedonism\",\n",
    "    ],\n",
    "    \"Self-enhancement\": [\n",
    "        \"Hedonism\",\n",
    "        \"Achievement\",\n",
    "        \"Power: dominance\",\n",
    "        \"Power: resources\",\n",
    "        \"Face\",\n",
    "    ],\n",
    "    \"Conservation\": [\n",
    "        \"Security: personal\",\n",
    "        \"Security: societal\",\n",
    "        \"Conformity: rules\",\n",
    "        \"Conformity: interpersonal\",\n",
    "        \"Tradition\",\n",
    "        \"Face\",\n",
    "        \"Humility\",\n",
    "    ],\n",
    "    \"Self-transcendence\": [\n",
    "        \"Benevolence: caring\",\n",
    "        \"Benevolence: dependability\",\n",
    "        \"Universalism: concern\",\n",
    "        \"Universalism: nature\",\n",
    "        \"Universalism: tolerance\",\n",
    "        \"Universalism: objectivity\",\n",
    "        \"Humility\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "column_to_drop = [x for l in level_3_to_2_mapping.values() for x in l]\n",
    "\n",
    "for category in level_3_categories:\n",
    "    # make a logical OR of all the level 2 categories\n",
    "    df_labels_test[category] = df_labels_test[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "    df_labels_val[category] = df_labels_val[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "    df_labels_train[category] = df_labels_train[level_3_to_2_mapping[category]].any(axis=1).map({True: 1, False: 0})\n",
    "\n",
    "df_labels_test = df_labels_test.drop(columns=column_to_drop)\n",
    "df_labels_val = df_labels_val.drop(columns=column_to_drop)\n",
    "df_labels_train = df_labels_train.drop(columns=column_to_drop)\n",
    "\n",
    "df_labels_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8316839c9de44b89",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Openness to change</th>\n",
       "      <th>Self-enhancement</th>\n",
       "      <th>Conservation</th>\n",
       "      <th>Self-transcendence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01002</td>\n",
       "      <td>We should ban human cloning</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01005</td>\n",
       "      <td>We should ban fast food</td>\n",
       "      <td>in favor of</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01006</td>\n",
       "      <td>We should end the use of economic sanctions</td>\n",
       "      <td>against</td>\n",
       "      <td>sometimes economic sanctions are the only thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01007</td>\n",
       "      <td>We should abolish capital punishment</td>\n",
       "      <td>against</td>\n",
       "      <td>capital punishment is sometimes the only optio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01008</td>\n",
       "      <td>We should ban factory farming</td>\n",
       "      <td>against</td>\n",
       "      <td>factory farming allows for the production of c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID                                   Conclusion       Stance  \\\n",
       "0      A01002                  We should ban human cloning  in favor of   \n",
       "1      A01005                      We should ban fast food  in favor of   \n",
       "2      A01006  We should end the use of economic sanctions      against   \n",
       "3      A01007         We should abolish capital punishment      against   \n",
       "4      A01008                We should ban factory farming      against   \n",
       "\n",
       "                                             Premise  Openness to change  \\\n",
       "0  we should ban human cloning as it will only ca...                   0   \n",
       "1  fast food should be banned because it is reall...                   0   \n",
       "2  sometimes economic sanctions are the only thin...                   0   \n",
       "3  capital punishment is sometimes the only optio...                   0   \n",
       "4  factory farming allows for the production of c...                   0   \n",
       "\n",
       "   Self-enhancement  Conservation  Self-transcendence  \n",
       "0                 0             1                   0  \n",
       "1                 0             1                   0  \n",
       "2                 1             1                   0  \n",
       "3                 0             1                   1  \n",
       "4                 0             1                   1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.merge(df_arg_train, df_labels_train, on='Argument ID')\n",
    "df_test = pd.merge(df_arg_test, df_labels_test, on='Argument ID')\n",
    "df_val = pd.merge(df_arg_val, df_labels_val, on='Argument ID')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdfe3378972543",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Task 1.5 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "232f6776a1ee9a80",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argument ID</th>\n",
       "      <th>Conclusion</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Premise</th>\n",
       "      <th>Openness to change</th>\n",
       "      <th>Self-enhancement</th>\n",
       "      <th>Conservation</th>\n",
       "      <th>Self-transcendence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A01002</td>\n",
       "      <td>We should ban human cloning</td>\n",
       "      <td>1</td>\n",
       "      <td>we should ban human cloning as it will only ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A01005</td>\n",
       "      <td>We should ban fast food</td>\n",
       "      <td>1</td>\n",
       "      <td>fast food should be banned because it is reall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A01006</td>\n",
       "      <td>We should end the use of economic sanctions</td>\n",
       "      <td>0</td>\n",
       "      <td>sometimes economic sanctions are the only thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A01007</td>\n",
       "      <td>We should abolish capital punishment</td>\n",
       "      <td>0</td>\n",
       "      <td>capital punishment is sometimes the only optio...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A01008</td>\n",
       "      <td>We should ban factory farming</td>\n",
       "      <td>0</td>\n",
       "      <td>factory farming allows for the production of c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argument ID                                   Conclusion  Stance  \\\n",
       "0      A01002                  We should ban human cloning       1   \n",
       "1      A01005                      We should ban fast food       1   \n",
       "2      A01006  We should end the use of economic sanctions       0   \n",
       "3      A01007         We should abolish capital punishment       0   \n",
       "4      A01008                We should ban factory farming       0   \n",
       "\n",
       "                                             Premise  Openness to change  \\\n",
       "0  we should ban human cloning as it will only ca...                   0   \n",
       "1  fast food should be banned because it is reall...                   0   \n",
       "2  sometimes economic sanctions are the only thin...                   0   \n",
       "3  capital punishment is sometimes the only optio...                   0   \n",
       "4  factory farming allows for the production of c...                   0   \n",
       "\n",
       "   Self-enhancement  Conservation  Self-transcendence  \n",
       "0                 0             1                   0  \n",
       "1                 0             1                   0  \n",
       "2                 1             1                   0  \n",
       "3                 0             1                   1  \n",
       "4                 0             1                   1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode stance into 0, 1 \n",
    "\n",
    "df_train[\"Stance\"] = df_train[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "df_test[\"Stance\"] = df_test[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "df_val[\"Stance\"] = df_val[\"Stance\"].map({\"in favor of\": 1, \"against\": 0})\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77076cfdcc028fa7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecad75b6100c0040",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ArgumentDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return {\n",
    "            \"Premise\": row[\"Premise\"],\n",
    "            \"Conclusion\": row[\"Conclusion\"],\n",
    "            \"labels\": torch.tensor(row[level_3_categories].values.tolist(), dtype=torch.float32),\n",
    "            \"stance\": torch.tensor(row[\"Stance\"], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94fe31d775254160",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Premise': 'we should ban human cloning as it will only cause huge issues when you have a bunch of the same humans running around all acting the same.', 'Conclusion': 'We should ban human cloning', 'labels': tensor([0., 0., 1., 0.]), 'stance': tensor(1.)}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ArgumentDataset(df_train)\n",
    "test_dataset = ArgumentDataset(df_test)\n",
    "val_dataset = ArgumentDataset(df_val)\n",
    "# Create the dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619bebf9aba984a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 3 Metric definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffa5c2876fecbf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task 2 Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd8b41505ded6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Random and Majority Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "331380fdc13c462f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomUniformClassifier(LightningModule):\n",
    "    def __init__(self):\n",
    "        self._random_state = np.random.RandomState()\n",
    "\n",
    "    def predict(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        logits = self._random_state.uniform(size=(batch_size, 4))\n",
    "        logits = logits > 0.5\n",
    "        return torch.tensor(logits, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class MajorityClassifier(LightningModule):\n",
    "    def __init__(self, n_random_classifiers=10):\n",
    "        self.n_random_classifiers = n_random_classifiers\n",
    "        self.random_classifiers = [RandomUniformClassifier() for _ in range(n_random_classifiers)]\n",
    "\n",
    "    def predict(self, X):\n",
    "        batch_size = X.shape[0]\n",
    "        votes = torch.zeros((batch_size, 4))\n",
    "        for clf in self.random_classifiers:\n",
    "            votes += clf.predict(X)\n",
    "        votes = votes / self.n_random_classifiers\n",
    "        votes = votes > 0.5\n",
    "        return torch.tensor(votes, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab6e80eded57efb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bert models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef0d525e8b3e87e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class BertConclusion(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded):\n",
    "        outputs = self.bert(**encoded)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "        X = data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded = self.tokenizer(X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        logits = self(encoded)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31dbbe5fe8eb3195",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BertPremiseConclusion(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded_1, encoded_2):\n",
    "        output_1 = self.bert(**encoded_1)\n",
    "        output_2 = self.bert(**encoded_2)\n",
    "\n",
    "        output = torch.cat((output_1.last_hidden_state[:, 0, :], output_2.last_hidden_state[:, 0, :]), dim=1)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2 = data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b11eb0f4965923dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BertPremiseConclusionStance(LightningModule):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 3, num_classes)\n",
    "\n",
    "        self.f1_metric = MultilabelF1Score(4, average=None)\n",
    "\n",
    "    def forward(self, encoded_1, encoded_2, encoded_3):\n",
    "        output_1 = self.bert(**encoded_1).last_hidden_state[:, 0, :]\n",
    "        output_2 = self.bert(**encoded_2).last_hidden_state[:, 0, :]\n",
    "        output_3 = self.bert(**encoded_3).last_hidden_state[:, 0, :]\n",
    "\n",
    "        output = torch.cat((output_1, output_2, output_3), dim=1)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, X_3 = data[\"Premise\"], data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_3 = self.tokenizer(X_3, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, encoded_3)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"train_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"train_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data = batch\n",
    "\n",
    "        X_1, X_2, X_3 = data[\"Premise\"], data[\"Conclusion\"], data[\"Conclusion\"]\n",
    "        y = data[\"labels\"]\n",
    "\n",
    "        encoded_1 = self.tokenizer(X_1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_2 = self.tokenizer(X_2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        encoded_3 = self.tokenizer(X_3, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        logits = self(encoded_1, encoded_2, encoded_3)\n",
    "\n",
    "        loss = nn.BCEWithLogitsLoss()(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        f1_score_per_class = self.f1_metric(logits, y)\n",
    "        f1_score_mean = torch.mean(f1_score_per_class)\n",
    "\n",
    "        self.log(\"val_f1_score\", f1_score_mean, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for i, category in enumerate(level_3_categories):\n",
    "            self.log(f\"val_f1_score_{category}\", f1_score_per_class[i], on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce3efa",
   "metadata": {},
   "source": [
    "Trainer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e71c67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    logger=TensorBoardLogger(\"lightning_logs\", name=\"bert\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(monitor=\"val_loss\"),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418deba6",
   "metadata": {},
   "source": [
    "F1 metric definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "308110ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = MultilabelF1Score(num_labels=4, average=None, multidim_average='global')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9494c9",
   "metadata": {},
   "source": [
    "Train all the models on the train set and evaluate them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565e094",
   "metadata": {},
   "source": [
    "1) BertConclusion Model: training, predictions, f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3805fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:630: Checkpoint directory lightning_logs\\bert\\version_2\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | bert       | BertModel         | 109 M \n",
      "1 | classifier | Linear            | 3.1 K \n",
      "2 | f1_metric  | MultilabelF1Score | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "437.941   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 123/169 [21:11<07:55,  0.10it/s, v_num=2, train_loss_step=0.596, train_f1_score_step=0.491, train_f1_score_Openness to change_step=0.000, train_f1_score_Self-enhancement_step=0.250, train_f1_score_Conservation_step=0.877, train_f1_score_Self-transcendence_step=0.836, val_loss=0.596, val_f1_score=0.516, val_f1_score_Openness to change=0.000, val_f1_score_Self-enhancement=0.328, val_f1_score_Conservation=0.854, val_f1_score_Self-transcendence=0.882, train_loss_epoch=0.606, train_f1_score_epoch=0.503, train_f1_score_Openness to change_epoch=0.000989, train_f1_score_Self-enhancement_epoch=0.288, train_f1_score_Conservation_epoch=0.863, train_f1_score_Self-transcendence_epoch=0.862]\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [01:30<00:00,  1.87it/s, v_num=2, train_loss_step=0.657, train_f1_score_step=0.403, train_f1_score_Openness to change_step=0.000, train_f1_score_Self-enhancement_step=0.000, train_f1_score_Conservation_step=0.828, train_f1_score_Self-transcendence_step=0.786, val_loss=0.621, val_f1_score=0.459, val_f1_score_Openness to change=0.000, val_f1_score_Self-enhancement=0.100, val_f1_score_Conservation=0.854, val_f1_score_Self-transcendence=0.882, train_loss_epoch=0.639, train_f1_score_epoch=0.524, train_f1_score_Openness to change_epoch=0.0306, train_f1_score_Self-enhancement_epoch=0.344, train_f1_score_Conservation_epoch=0.860, train_f1_score_Self-transcendence_epoch=0.862]\n"
     ]
    }
   ],
   "source": [
    "bertConclusion= BertConclusion(\"bert-base-uncased\", num_classes=len(level_3_categories))\n",
    "\n",
    "trainer.fit(bertConclusion, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5f14417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1576, 4])\n"
     ]
    }
   ],
   "source": [
    "predictions_bertConclusion = utils.model_predict_1(bertConclusion, test_dataloader)\n",
    "print(predictions_bertConclusion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4da3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Openness to change', 'Self-enhancement', 'Conservation', 'Self-transcendence']\n",
    "df_labels_test = df_labels_test[selected_columns]\n",
    "target_tensor = torch.tensor(df_labels_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "060e2210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for each class:\n",
      "Openness to change: 0.0084\n",
      "Self-enhancement: 0.4011\n",
      "Conservation: 0.8304\n",
      "Self-transcendence: 0.8917\n",
      "Overall F1 Score: 0.5329\n"
     ]
    }
   ],
   "source": [
    "results_bConclusion = f1_metric(predictions_bertConclusion, target_tensor)\n",
    "average_bConclusion = sum(results_bConclusion)/4\n",
    "print(\"F1 Score for each class:\")\n",
    "for i, category in enumerate(level_3_categories):\n",
    "    print(f\"{category}: {results_bConclusion[i]:.4f}\")\n",
    "\n",
    "print(f\"Overall F1 Score: {average_bConclusion:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc4479",
   "metadata": {},
   "source": [
    "2) BertPremiseConclusion Model: training, predictions, f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b7e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | bert       | BertModel         | 109 M \n",
      "1 | classifier | Linear            | 6.1 K \n",
      "2 | f1_metric  | MultilabelF1Score | 0     \n",
      "-------------------------------------------------\n",
      "6.1 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "437.954   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [02:13<00:00,  1.26it/s, v_num=1, train_loss_step=0.663, train_f1_score_step=0.464, train_f1_score_Openness to change_step=0.000, train_f1_score_Self-enhancement_step=0.333, train_f1_score_Conservation_step=0.938, train_f1_score_Self-transcendence_step=0.583, val_loss=0.600, val_f1_score=0.553, val_f1_score_Openness to change=0.000, val_f1_score_Self-enhancement=0.475, val_f1_score_Conservation=0.854, val_f1_score_Self-transcendence=0.882, train_loss_epoch=0.609, train_f1_score_epoch=0.511, train_f1_score_Openness to change_epoch=0.000848, train_f1_score_Self-enhancement_epoch=0.320, train_f1_score_Conservation_epoch=0.862, train_f1_score_Self-transcendence_epoch=0.861]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 169/169 [02:16<00:00,  1.24it/s, v_num=1, train_loss_step=0.663, train_f1_score_step=0.464, train_f1_score_Openness to change_step=0.000, train_f1_score_Self-enhancement_step=0.333, train_f1_score_Conservation_step=0.938, train_f1_score_Self-transcendence_step=0.583, val_loss=0.600, val_f1_score=0.553, val_f1_score_Openness to change=0.000, val_f1_score_Self-enhancement=0.475, val_f1_score_Conservation=0.854, val_f1_score_Self-transcendence=0.882, train_loss_epoch=0.609, train_f1_score_epoch=0.511, train_f1_score_Openness to change_epoch=0.000848, train_f1_score_Self-enhancement_epoch=0.320, train_f1_score_Conservation_epoch=0.862, train_f1_score_Self-transcendence_epoch=0.861]\n"
     ]
    }
   ],
   "source": [
    "bertPremiseConclusion= BertPremiseConclusion(\"bert-base-uncased\", num_classes=len(level_3_categories))\n",
    "\n",
    "trainer.fit(bertPremiseConclusion, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f60e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1576, 4])\n"
     ]
    }
   ],
   "source": [
    "predictions_bertPremiseConclusion = utils.model_predict_2(bertPremiseConclusion, test_dataloader)\n",
    "print(predictions_bertPremiseConclusion.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e2352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for each class:\n",
      "Openness to change: 0.0000\n",
      "Self-enhancement: 0.2747\n",
      "Conservation: 0.8304\n",
      "Self-transcendence: 0.8917\n",
      "Overall F1 Score: 0.4992\n"
     ]
    }
   ],
   "source": [
    "results_bPC = f1_metric(predictions_bertPremiseConclusion, target_tensor)\n",
    "average_bPC = sum(results_bPC)/4\n",
    "print(\"F1 Score for each class:\")\n",
    "for i, category in enumerate(level_3_categories):\n",
    "    print(f\"{category}: {results_bPC[i]:.4f}\")\n",
    "\n",
    "print(f\"Overall F1 Score: {average_bPC:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058054af",
   "metadata": {},
   "source": [
    "3) BertPremiseConclusionStance Model: training, predictions, f1 metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ae8c52cc463208",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | bert       | BertModel         | 109 M \n",
      "1 | classifier | Linear            | 9.2 K \n",
      "2 | f1_metric  | MultilabelF1Score | 0     \n",
      "-------------------------------------------------\n",
      "9.2 K     Trainable params\n",
      "109 M     Non-trainable params\n",
      "109 M     Total params\n",
      "437.966   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 123/169 [04:37<01:43,  0.44it/s, v_num=2, train_loss_step=0.596, train_f1_score_step=0.491, train_f1_score_Openness to change_step=0.000, train_f1_score_Self-enhancement_step=0.250, train_f1_score_Conservation_step=0.877, train_f1_score_Self-transcendence_step=0.836, val_loss=0.596, val_f1_score=0.516, val_f1_score_Openness to change=0.000, val_f1_score_Self-enhancement=0.328, val_f1_score_Conservation=0.854, val_f1_score_Self-transcendence=0.882, train_loss_epoch=0.606, train_f1_score_epoch=0.503, train_f1_score_Openness to change_epoch=0.000989, train_f1_score_Self-enhancement_epoch=0.288, train_f1_score_Conservation_epoch=0.863, train_f1_score_Self-transcendence_epoch=0.862] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "bertClassifier = BertPremiseConclusionStance(\"bert-base-uncased\", num_classes=len(level_3_categories))\n",
    "\n",
    "trainer.fit(bertClassifier, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09020c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1576, 4])\n"
     ]
    }
   ],
   "source": [
    "predictions_bertClassifier = utils.model_predict_3(bertClassifier, test_dataloader)\n",
    "print(predictions_bertClassifier.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651eb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for each class:\n",
      "Openness to change: 0.0000\n",
      "Self-enhancement: 0.3473\n",
      "Conservation: 0.8304\n",
      "Self-transcendence: 0.8917\n",
      "Overall F1 Score: 0.5174\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results_bC = f1_metric(predictions_bertClassifier, target_tensor)\n",
    "average_bC = sum(results_bC)/4\n",
    "print(\"F1 Score for each class:\")\n",
    "for i, category in enumerate(level_3_categories):\n",
    "    print(f\"{category}: {results_bC[i]:.4f}\")\n",
    "\n",
    "print(f\"Overall F1 Score: {average_bC:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
