{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning\n",
    "# !pip install torchtext.data\n",
    "# !pip install torchtext\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove unused dependencies\n",
    "\n",
    "# file management\n",
    "import sys\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# DL framework\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "# dataframe management\n",
    "import pandas as pd\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "\n",
    "# for readability\n",
    "from typing import Iterable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "\n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_file:\n",
    "        zip_file.extractall(extract_path)\n",
    "\n",
    "    print(\"Extraction completed!\")\n",
    "\n",
    "    Path.unlink(download_path)\n",
    "    print(\"Deleted .zip dataset file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: c:\\Users\\merli\\OneDrive\\Desktop\\Github repos\\NLP\\A1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_zip_path.exists():\n",
    "    download_dataset(dataset_zip_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_zip_path, dataset_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the corpus into a pandas.DataFrame object and split it into train, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains 200 documents.\n",
    "\n",
    "   * **Train**: Documents 1-100\n",
    "   * **Validation**: Documents 101-150\n",
    "   * **Test**: Documents 151-199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_rows = []  # list for DataFrame rows\n",
    "id = 0\n",
    "\n",
    "for i, file_path in enumerate(sorted(dataset_path.iterdir())):\n",
    "    if file_path.is_file():  # split corpus documents in the tree categories: train, validation, tests\n",
    "        if 1 <= i + 1 <= 100:\n",
    "            split = 'train'\n",
    "        elif 101 <= i + 1 <= 150:\n",
    "            split = 'validation'\n",
    "        else:\n",
    "            split = 'test'\n",
    "\n",
    "        with file_path.open(mode='r', encoding='utf-8') as text_file:  # read corpus lines\n",
    "            lines = text_file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) == 1:\n",
    "                id = id + 1\n",
    "            if len(fields) >= 2:\n",
    "                text = fields[0]  # store the first field as 'text'\n",
    "                POS = fields[1]  # store the second field as 'POS'\n",
    "                dataframe_row = {  #build DataFrame rows\n",
    "                    \"text\": text,\n",
    "                    \"POS\": POS,\n",
    "                    \"split\": split,\n",
    "                    \"id\": id\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row)  #append rows\n",
    "# corpus DataFrame\n",
    "corpus_df = pd.DataFrame(dataframe_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## TASK 1: Corpus\n",
    "\n",
    "* **Download** the corpus.\n",
    "* **Encode** the corpus into a pandas.DataFrame object.\n",
    "* **Split** it in training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "      <th>split</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>old</td>\n",
       "      <td>JJ</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>will</td>\n",
       "      <td>MD</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>join</td>\n",
       "      <td>VB</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text  POS  split  id\n",
       "0  Pierre  NNP  train   0\n",
       "1  Vinken  NNP  train   0\n",
       "2       ,    ,  train   0\n",
       "3      61   CD  train   0\n",
       "4   years  NNS  train   0\n",
       "5     old   JJ  train   0\n",
       "6       ,    ,  train   0\n",
       "7    will   MD  train   0\n",
       "8    join   VB  train   0\n",
       "9     the   DT  train   0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test, validation split\n",
    "df_train = corpus_df[corpus_df['split'] == 'train'].drop(columns=['split'])\n",
    "df_test = corpus_df[corpus_df['split'] == 'test'].drop(columns=['split'])\n",
    "df_val = corpus_df[corpus_df['split'] == 'validation'].drop(columns=['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe structure:\n",
      "          text  POS  split    id\n",
      "0       Pierre  NNP  train     0\n",
      "1       Vinken  NNP  train     0\n",
      "2            ,    ,  train     0\n",
      "3           61   CD  train     0\n",
      "4        years  NNS  train     0\n",
      "...        ...  ...    ...   ...\n",
      "94079  quarter   NN   test  3715\n",
      "94080       of   IN   test  3715\n",
      "94081     next   JJ   test  3715\n",
      "94082     year   NN   test  3715\n",
      "94083        .    .   test  3715\n",
      "\n",
      "[94084 rows x 4 columns]\n",
      "\n",
      "Total rows 94084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe structure:\")\n",
    "print(corpus_df)\n",
    "print()\n",
    "\n",
    "print(f\"Total rows {len(corpus_df)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Text encoding\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* TODO: see if we want to do it, otherwise remove it -> [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed words unsing GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe, build_vocab_from_iterator\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(embedding_dimension: int = 300):\n",
    "    emb_model = GloVe(name=\"6B\", dim=embedding_dimension)\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation_and_symbol_pos = [\".\", \",\", \":\", '``', \"''\", \"$\", \"#\", \"-LRB-\", \"-RRB-\", \"SYM\", \"LS\"] #TODO check \"LS\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = ([pos] for pos in corpus_df[\"POS\"].unique())\n",
    "pos_vocab = build_vocab_from_iterator(iterator)\n",
    "pos_vocab.append_token(\"<PAD>\")\n",
    "\n",
    "pos_padding_value = pos_vocab[\"<PAD>\"]\n",
    "punctuation_and_symbol_pos_indices = [pos_vocab[token] for token in punctuation_and_symbol_pos]\n",
    "\n",
    "\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, embedder):\n",
    "        min_id = dataframe['id'].min()\n",
    "        dataframe['id'] = dataframe['id'] - min_id\n",
    "        self.dataframe = dataframe.groupby(\"id\")\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.dataframe.get_group(idx)\n",
    "        text = sentence['text'].to_list()\n",
    "        text = [token.lower() for token in text]        \n",
    "        \n",
    "        POS = sentence['POS'].to_list()\n",
    "        POS = torch.Tensor([pos_vocab[token] for token in POS])\n",
    "\n",
    "        # POS_one_hot = torch.nn.functional.one_hot(POS.to(torch.int64), num_classes=len(pos_vocab))\n",
    "        # TODO: remove\n",
    "        embedded_text = self.embedder.get_vecs_by_tokens(text)\n",
    "\n",
    "        return embedded_text, POS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the dataset\n",
    "EMBEDDING_DIM = 50\n",
    "embedder = load_embedding_model(EMBEDDING_DIM)\n",
    "dataset_train = CorpusDataset(df_train, embedder)\n",
    "dataset_test = CorpusDataset(df_test, embedder)\n",
    "dataset_val = CorpusDataset(df_val, embedder)\n",
    "dataset_all = CorpusDataset(corpus_df, embedder)\n",
    "\n",
    "\n",
    "# TODO - test if it works in the LSTM training\n",
    "def my_collate(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    # max_len = max([len(seq) for seq in sequences])\n",
    "    # sequences_padded = [seq + [\"<PAD>\"] * (max_len - len(seq)) for seq in sequences]\n",
    "\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=pos_padding_value)\n",
    "\n",
    "    sequences_padded = sequences_padded.type(torch.float)\n",
    "    labels_padded = labels_padded.type(torch.long)\n",
    "\n",
    "    return [sequences_padded, labels_padded]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=32, collate_fn=my_collate)\n",
    "val_loader = DataLoader(dataset_val, batch_size=32, collate_fn=my_collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=32, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Model definition\n",
    "\n",
    "* **Baseline**: implement a Bidirectional LSTM with a Dense layer on top.\n",
    "\n",
    "* **Model 1**: add an additional LSTM layer to the Baseline model.\n",
    "* **Model 2**: add an additional Dense layer to the Baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model: Bidirectional LSTM + Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torchmetrics import Metric\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "\n",
    "class F1ScoreCustom(Metric):\n",
    "    def __init__(self, num_classes: int, pos_padding_value: int = pos_padding_value, punctuation_and_symbol_pos_indices: list = punctuation_and_symbol_pos_indices):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.mask = torch.ones([num_classes])\n",
    "        self.mask[[pos_padding_value] + punctuation_and_symbol_pos_indices] = 0\n",
    "        \n",
    "        self.add_state(\"true_positive\", default=torch.zeros([num_classes]), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"false_negative\", default=torch.zeros([num_classes]), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"false_positive\", default=torch.zeros([num_classes]), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, y_hat_class: torch.Tensor, y_class: torch.Tensor):\n",
    "        confusion_matrix_metric = ConfusionMatrix(num_classes=self.num_classes, task=\"multiclass\")\n",
    "        confusion_matrix = confusion_matrix_metric(y_hat_class, y_class)\n",
    "\n",
    "        # # Confusion matrix, TP, FN and FP for class 0 \n",
    "        # #   TRUE LABEL\n",
    "        # #   0               TP     FN     FN     FN     FN       \n",
    "        # #   1               FP       \n",
    "        # #   2               FP               \n",
    "        # #   3               FP                       \n",
    "        # #   4               FP                                \n",
    "        # # PREDICTED LABEL   0       1      2      3      4\n",
    "        # \n",
    "\n",
    "        true_positive = torch.Tensor([confusion_matrix[i][i] for i in range(self.num_classes)])\n",
    "        false_negative = torch.Tensor([sum(confusion_matrix[i, :]) - true_positive[i] for i in range(self.num_classes)])\n",
    "        false_positive = torch.Tensor([sum(confusion_matrix[:, i]) - true_positive[i] for i in range(self.num_classes)])\n",
    "\n",
    "        self.true_positive += true_positive\n",
    "        self.false_negative += false_negative\n",
    "        self.false_positive += false_positive\n",
    "\n",
    "    def compute(self):\n",
    "        precision = self.true_positive / (self.true_positive + self.false_positive)\n",
    "        recall = self.true_positive / (self.true_positive + self.false_negative)\n",
    "\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        f1 = f1 * self.mask\n",
    "\n",
    "        return f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, target_padding_value=pos_padding_value):  #, embedder):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.target_padding_value = target_padding_value\n",
    "\n",
    "        # self.embedder = embedder\n",
    "        # self.embedding_layer = nn.Embedding.from_pretrained(embedder.vectors)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiplied by 2 due to the bidirectionality\n",
    "\n",
    "        self._train_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "        self._val_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "        self._test_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedding = self.embedding_layer(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out (batch_size, seq_length, hidden_size * 2)\n",
    "        out = self.fc(lstm_out)\n",
    "        # out (batch_size, seq_length, output_dim)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # Change shape from (batchsize, sequence_len, classes) to be (batchsize, classes, sequence_len) to compute loss function\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "\n",
    "        self.log_dict({'train_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      on_step=False, reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._train_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "        self.log_dict({'val_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._val_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "\n",
    "        self.log_dict({'test_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._test_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def _compute_f1(self, f1_metric):\n",
    "        mean_f1_score = f1_metric.compute()\n",
    "\n",
    "        # Create a mask that is False for NaNs\n",
    "        mask = torch.isnan(mean_f1_score)\n",
    "\n",
    "        # Invert the mask: True for valid entries, False for NaNs\n",
    "        valid_data = mean_f1_score[~mask]\n",
    "\n",
    "        # Compute the mean of the non-NaN values\n",
    "        mean_value = torch.mean(valid_data)\n",
    "\n",
    "        return mean_value\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._train_f1_metric)\n",
    "        self.log_dict({\"train_f1\": mean_f1_score, 'step': float(self.current_epoch)}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._train_f1_metric.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._val_f1_metric)\n",
    "        self.log_dict({\"val_f1\": mean_f1_score, 'step': float(self.current_epoch)}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._val_f1_metric.reset()\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._test_f1_metric)\n",
    "        self.log_dict({\"test_f1\": mean_f1_score, 'step': float(self.current_epoch)}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._test_f1_metric.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dim = len(df_train[\"POS\"].unique()) + 1  # +1 for padding\n",
    "input_dim = EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "hidden_dim = 128\n",
    "max_epochs = 50\n",
    "\n",
    "save_dir = \"logs\"\n",
    "load_model = False\n",
    "train_model = True\n",
    "model_ckpt = \"baseline.ckpt\"\n",
    "\n",
    "PATH = os.path.join(save_dir, \"lightning_logs\", \"version_\" + str(baseline_version), \"checkpoints\", model_ckpt)\n",
    "\n",
    "if load_model:\n",
    "    model = BiLSTMModel.load_from_checkpoint(PATH, input_dim=input_dim, hidden_dim=hidden_dim,\n",
    "                                             output_dim=output_dim)  #, embedder=embedder)\n",
    "else:\n",
    "    model = BiLSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)  #, embedder=embedder)\n",
    "\n",
    "if train_model:\n",
    "    tb_logger = TensorBoardLogger(version=baseline_version, save_dir='logs')\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, logger=tb_logger, default_root_dir='./', log_every_n_steps=1)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.save_checkpoint(PATH)\n",
    "    baseline_version += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "validation_results_baseline_model = trainer.validate(model, dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Bidirectional 2-layers LSTM + Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, target_padding_value=pos_padding_value):  #, embedder):\n",
    "        super(Model1, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.target_padding_value = target_padding_value\n",
    "\n",
    "        # self.embedder = embedder\n",
    "        # self.embedding_layer = nn.Embedding.from_pretrained(embedder.vectors)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiplied by 2 due to the bidirectionality\n",
    "\n",
    "        self._train_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "        self._val_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "        self._test_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedding = self.embedding_layer(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out (batch_size, seq_length, hidden_size * 2)\n",
    "        out = self.fc(lstm_out)\n",
    "        # out (batch_size, seq_length, output_dim)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # Change shape from (batchsize, sequence_len, classes) to be (batchsize, classes, sequence_len) to compute loss function\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "\n",
    "        self.log_dict({'train_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      on_step=False, reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._train_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "        self.log_dict({'val_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._val_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "\n",
    "        self.log_dict({'test_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._test_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def _compute_f1(self, f1_metric):\n",
    "        mean_f1_score = f1_metric.compute()\n",
    "\n",
    "        # Create a mask that is False for NaNs\n",
    "        mask = torch.isnan(mean_f1_score)\n",
    "\n",
    "        # Invert the mask: True for valid entries, False for NaNs\n",
    "        valid_data = mean_f1_score[~mask]\n",
    "\n",
    "        # Compute the mean of the non-NaN values\n",
    "        mean_value = torch.mean(valid_data)\n",
    "\n",
    "        return mean_value\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._train_f1_metric)\n",
    "        self.log_dict({\"train_f1\": mean_f1_score, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._train_f1_metric.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._val_f1_metric)\n",
    "        self.log_dict({\"val_f1\": mean_f1_score, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._val_f1_metric.reset()\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._test_f1_metric)\n",
    "        self.log_dict({\"test_f1\": mean_f1_score, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._test_f1_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 28\n",
    "max_epochs = 50\n",
    "\n",
    "save_dir = \"logs\"\n",
    "load_model = False\n",
    "train_model = True\n",
    "model_ckpt = \"model1.ckpt\"\n",
    "\n",
    "PATH = os.path.join(save_dir, \"lightning_logs\", \"version_\" + str(model1_version), \"checkpoints\", model_ckpt)\n",
    "\n",
    "if load_model:\n",
    "    model_1 = Model1.load_from_checkpoint(PATH, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "else:\n",
    "    model_1 = Model1(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "if train_model:\n",
    "    tb_logger = TensorBoardLogger(version=model1_version, save_dir='logs')\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, logger=tb_logger, default_root_dir='./', log_every_n_steps=1)\n",
    "    trainer.fit(model_1, train_loader, val_loader)\n",
    "    trainer.save_checkpoint(PATH)\n",
    "    model1_version += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_results_model_1 = trainer.validate(model_1, dataloaders=val_loader);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Bidirectional LSTM + 2 Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model2(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, fc_size,\n",
    "                 target_padding_value=pos_padding_value):  #, embedder):\n",
    "        super(Model2, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.target_padding_value = target_padding_value\n",
    "\n",
    "        # self.embedder = embedder\n",
    "        # self.embedding_layer = nn.Embedding.from_pretrained(embedder.vectors)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim * 2, fc_size)  # Multiplied by 2 due to bidirectionality\n",
    "        self.fc_2 = nn.Linear(fc_size, output_dim)\n",
    "\n",
    "        self._train_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "        self._val_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "        self._test_f1_metric = F1ScoreCustom(num_classes=output_dim, pos_padding_value=self.target_padding_value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # embedding = self.embedding_layer(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out (batch_size, seq_length, hidden_size * 2)\n",
    "        out = self.fc_1(lstm_out)\n",
    "        out = self.fc_2(out)\n",
    "        # out (batch_size, seq_length, output_dim)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # Change shape from (batchsize, sequence_len, classes) to be (batchsize, classes, sequence_len) to compute loss function\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "\n",
    "        self.log_dict({'train_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      on_step=False, reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._train_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "        self.log_dict({'val_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._val_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y, ignore_index=self.target_padding_value)\n",
    "\n",
    "        self.log_dict({'test_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True,\n",
    "                      reduce_fx=\"mean\")\n",
    "\n",
    "        y_hat_class = torch.argmax(y_hat, dim=1)\n",
    "        self._test_f1_metric.update(y_hat_class, y)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    def _compute_f1(self, f1_metric):\n",
    "        mean_f1_score = f1_metric.compute()\n",
    "\n",
    "        # Create a mask that is False for NaNs\n",
    "        mask = torch.isnan(mean_f1_score)\n",
    "\n",
    "        # Invert the mask: True for valid entries, False for NaNs\n",
    "        valid_data = mean_f1_score[~mask]\n",
    "\n",
    "        # Compute the mean of the non-NaN values\n",
    "        mean_value = torch.mean(valid_data)\n",
    "\n",
    "        return mean_value\n",
    "\n",
    "    def on_train_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._train_f1_metric)\n",
    "        self.log_dict({\"train_f1\": mean_f1_score, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._train_f1_metric.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._val_f1_metric)\n",
    "        self.log_dict({\"val_f1\": mean_f1_score, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._val_f1_metric.reset()\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        mean_f1_score = self._compute_f1(self._test_f1_metric)\n",
    "        self.log_dict({\"test_f1\": mean_f1_score, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self._test_f1_metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_dim = 28\n",
    "fc_size = 100\n",
    "\n",
    "max_epochs = 10\n",
    "\n",
    "save_dir = \"logs\"\n",
    "load_model = False\n",
    "train_model = True\n",
    "model_ckpt = \"model2.ckpt\"\n",
    "\n",
    "PATH = os.path.join(save_dir, \"lightning_logs\", \"version_\" + str(model2_version), \"checkpoints\", model_ckpt)\n",
    "\n",
    "if load_model:\n",
    "    model_2 = Model2.load_from_checkpoint(PATH, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim,\n",
    "                                          fc_size=fc_size)\n",
    "else:\n",
    "    model_2 = Model2(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, fc_size=fc_size)\n",
    "\n",
    "if train_model:\n",
    "    tb_logger = TensorBoardLogger(version=model2_version, save_dir='logs')\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, logger=tb_logger, default_root_dir='./', log_every_n_steps=1)\n",
    "    trainer.fit(model_2, train_loader, val_loader)\n",
    "    trainer.save_checkpoint(PATH)\n",
    "    model2_version += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_results_model_2 = trainer.validate(model_2, dataloaders=val_loader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Metrics\n",
    "\n",
    "* Assign static embedding to validation and test set OOV words (Edo: see unk_init param at [docs](https://torchtext.readthedocs.io/en/latest/vocab.html))\n",
    "* **Concatenate** all tokens in a data split -> (**Hint**: accumulate FP, TP, FN, TN iteratively) TODO: rewrite \n",
    "* Evaluate models using macro F1-score, computed over **all** tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-vocabulary words in training set: {'synergistics', 'forest-product', 'recession-inspired', 'sacramento-based', 'sticker-shock', 'built-from-kit', 'teacher-cadet', 'durable-goods', 'samnick', 'univest', 'capital-gains', '449.04', '82,389', 'stirlen', 'thin-lipped', '-lcb-', 'abortion-related', 'home-market', 'trockenbeerenauslesen', 'anti-takeover', 'church-goers', 'search-and-seizure', 'rapanelli', 'delwin', 'pramual', 'morale-damaging', 'money-fund', '100,980', 'intellectual-property', '456.64', 'glenham', '84-month', '-rcb-', 'corton-charlemagne', '-lrb-', '374.20', 'ariail', 'machine-gun-toting', 'ntg', 'coche-dury', 'buttoned-down', 'subminimum', '374.19', 'shirt-sleeved', 'wfrr', 'sanderoff', 'auto-safety', 'yeargin', 'chilver', 'tiphook', 'centerbank', 'pre-1933', 'subskills', 'test-preparation', 'purepac', 'nearly-30', 'low-ball', 'post-hearing', 'ac-130u', 'mouth-up', 'market-share', 'non-biodegradable', 'cotran', 're-thought', '13,056', 'chemplus', 'sub-markets', 'safe-deposit', 'energy-services', 'direct-investment', '3\\\\/4', 'pathlogy', 'retin-a', 'romanee-conti', 'ratners', 'school-improvement', 'two-sevenths', 'antitrust-law', 'anti-china', 'identity-management', 'deposits-a', 'unfair-trade', '143.08', 'bumkins', 'rubinfien', 'building-products', 'unenticing', 'social-studies', 'nekoosa', 'red-blooded', '90-cent-an-hour', \"creator's\", 'high-rate', '37-a-share', 'sharedata', 'ghkm', '2645.90', 'micronite', '4,393,237', 'state-supervised', 'subskill', 'three-sevenths', '18,444', 'when-issued', 'water-authority', 'life-insurance', 'savers\\\\/investors', 'limited-partnership', '9,118', '415.8', 'ctbs', 'weisfield', 'electric-utility', 'mehrens', '1\\\\/2', '16,072', 'anti-abortionists', 'low-ability', 'drag-down', 'senate-house', '127.03', 'bridgestone\\\\/firestone', 'three-lawyer', '1.457', 'satrum', 'chinchon', 'flightiness', '236.79', 'co-developers', 'ingersoll-rand', 'sport-utility', 'industrial-production', 'big-ticket', 'lower-priority', 'test-practice', 'bellringers', 'rexinger', '38.375', '62%-owned', 'computer-driven', 'muscolina', 'war-rationed', 'sometimes-exhausting', '1.5755', 'security-type', 'superpremiums', 'roof-crush', 'language-housekeeper', 'bald-faced', 'iran\\\\/contra', 'malizia', 'telephone-information', 'equal-opportunity', 'macmillan\\\\/mcgraw', 'year-ago', 'sino-u.s.', 'drobnick', '446.62', '236.74', '497.34', 'one-country', 'one-yen', 'chafic', 'chong-sik', 'index-related', 'mininum-wage', 'foreign-stock', '500,004', '62.625', 'yen-support', 'greenmailer', 'light-truck', '8300s', 'makato', 'custom-chip', 'one-upsmanship', 'incentive-bonus', 'lafite-rothschild', 'program-trading', 'nipponese', 'computer-system-design', 'we-japanese', 'akerfeldt', 'c-90', 'meinders', 'high-balance', '11,762', '30,841', 'side-crash', '70-a-share', 'rate-sensitive', 'foreign-led', 'trading-company', '415.6', 'wheel-loader', 'integra-a', 'food-shop', 'mortgage-based', 'dollar-yen', 'index-options', 'pattenden', 'test-coaching', '520-lawyer', 'marketing-communications', 'collective-bargaining', 'pianist-comedian', 'veselich', 'ensrud', 'hallwood', 'derel', 'hummerstone', 'small-company', '352.7', 'beer-belly', 'eight-count', 'ft-se', 'test-prep', '16.125', 'junk-bond', '3.253', '14,821', '361,376', '737.5', 'front-seat', 'lezovich', 'polyproplene', 'student-test', 'car-safety', 'landonne', '95,142', '142.85', 'jerritts', 'change-ringing', '278.7', 'boorse', '230-215', 'besuboru', 'aslacton', 'money-center', 'moleculon', 'subindustry', 'sub-segments', 'futures-related', 'tarwhine', 'gingl', 'stock-manipulation', 'video-viewing', 'training-wage', '1\\\\/4', 'preparatives', 'times-stock', 'bell-ringer', '2,303,328', 'uzi-model', 'family-planning', 'school-district', 'highest-pitched', 'summer\\\\/winter', 'alurralde', '271,124', 'amphobiles', 'monchecourt', 'red-flag', 'vitulli', '1\\\\/10th', '1.8415', 'twindam', 'purhasing', 'cray-3', 'biondi-santi', 'sogo-shosha', 'macheski', '12,252', '-rrb-', 'dead-eyed', 'fetal-tissue', '382-37', 'macmillan\\\\/mcgraw-hill', '143.80', 'college-bowl', 'nagymaros', 'money-market', '3057', 'erbamont', 'solaia', 'investor-relations', 'higher-salaried', 'kalipharma', 'merger-related', 'secilia', '7\\\\/8', 'achievement-test', 'crocidolite', 'less-serious', 'page-one', 'lap-shoulder', 'top-yielding', 'wine-buying', 'odd-sounding', 'floating-rate', 'six-packs', 'stock-index', 'twin-jet', 'year-earlier', '234.4', 'northy', 'norwick', 'new-home', 'nih-appointed', 'product-design', 'dust-up', 'sometimes-tawdry', 'asset-sale', 'jalaalwalikraam', '3,288,453', 'school-board', 'securities-based', 'tissue-transplant', 'non-encapsulating', '5.276', 'revenue-desperate', 'rope-sight', 'trettien', 'old-house', 'nesb', '436.01', 'cash-rich', 'forest-products', 'yen-denominated', 'c.j.b.', '143.93', 'six-bottle', 'more-efficient', 'vinken', 'circuit-breaker', 'cop-killer', 'wheeland', '705.6', 'pennview', '69-point', 'sell-offs', 'nissho-iwai', 'pro-forma', 'index-arbitrage', 'savings-and-loan', 'colonsville', 'automotive-parts', 'wtd', 'automotive-lighting', 'replacement-car', 'detective-story', '5\\\\/8', 'bermuda-based', 'government-certified', 'autions', '4.898', 'prize-fighter', 'pre-1917', 'school-research'}\n",
      "len: 359\n",
      "Out-of-vocabulary words in validation set: {'2160.1', 'motor-home', 'ballantine\\\\/del', 'gates-warren', '6\\\\/2', 'investment-grade', '1937-40', 'heiwado', 'labor-backed', 'test-drive', 'less-than-brilliant', '8.575', '8.467', 'herald-american', 'sulfur-dioxide', 'money-market', 'multi-crystal', '14\\\\/32', 'million-a-year', 'egnuss', '238,000-circulation', 'parts-engineering', '2141.7', '352.9', 'stock-specialist', 'nylev', 'mega-stadium', '-lcb-', '30,537', 'non-callable', '7\\\\/8', 'n.v', 'anti-programmers', 'prevalance', 'ft-se', '95.09', 'yttrium-containing', '2003\\\\/2007', 'bottom-line', 'index-fund', 'house-senate', 'lynch-mob', 'buy-outs', 'wamre', 'bank-backed', 'four-foot-high', 'junk-bond', 'boogieman', '3\\\\/8', 'prudential-bache', 'veraldi', 'super-absorbent', 'floating-rate', 'mutchin', 'dydee', '1\\\\/2', 'sidak', 'stock-index', 'six-packs', 'arighi', '7.458', 'property\\\\/casualty', 'certin', 'express-buick', 'melt-textured', 'two-time-losers', 'one-newspaper', 'long-tenured', 'year-earlier', '877,663', 'single-lot', 'capital-markets', 'price-depressing', 'substance-abusing', 'crystal-lattice', '-rcb-', 'bridgestone\\\\/firestone', 'larger-than-normal', 'liquid-nitrogen', 'quantitive', '-lrb-', 'severable', 'financial-services', 'stock-picking', 'diceon', 'food-industry', '20-stock', 'potables', '372.9', 'executive-office', 'land-idling', '3,250,000', 'then-speaker', 'car-development', '13\\\\/16', 'cup-tote', '23,403', '35564.43', '71,309', 'scypher', 'circuit-board', '7.272', 'ednie', 'write-downs', 'freudtoy', 'crane-safety', 'flim-flammery', 'one-house', 'free-enterprise', '14.', 'incentive-backed', '29year', 'band-wagon', 'anku', 'much-larger', 'anti-program', 'credit-rating', '1928-33', 'midwesco', '3,040,000', '62%-owned', 'equity-purchase', '7.422', 'computer-driven', '11\\\\/16', 'seven-million-ton', '300-a-share', '300-113', 'truth-in-lending', 'lookee-loos', 'card-member', 'continuingly', '2163.2', '1\\\\/4', '7\\\\/16', '271-147', 'most-likely-successor', 'market-share', 'industry-supported', 'stock-price', 'disaster-assistance', 'anti-deficiency', 'triple-a-rated', 'minicrash', 'times-stock', 'clean-air', 'pension-fund', 'rey\\\\/fawcett', 'takeover-stock', 'housing-assistance', 'acid-rain', '47.125', 'year-ago', 'propagandizes', '9\\\\/32', 'corporate-wide', '3\\\\/4', 'car-care', 'enzor', '1738.1', '2\\\\/32', 'cleaner-burning', 'airline-related', '16\\\\/32', 'radio-station', '3648.82', 'anti-morning-sickness', 'fiber-end', '12\\\\/32', 'arbitraging', 'superdot', 'new-car', '13.625', 'foldability', 'pricings', 'citizen-sparked', 'double-c', 'index-arbitrage', 'middle-ground', 'stock-selection', '5.435', '22\\\\/32', 'triple-c', 'hart-scott-rodino', 'shokubai', 'price-support', '1\\\\/8', 'anti-miscarriage', 'intecknings', 'freshbake', '35500.64', 'program-trading', 'news-american', 'mutual-fund', 'tire-kickers', 'profit-taking', 'walbrecher', 'market-makers', 'high-polluting', 'breakey', '-rrb-', 'contingency-fee', 'electrical-safety', 'odd-year', '190-point', '5\\\\/8', 'phacoflex', 'heebie-jeebies', '2691.19', 'insider-trading', 'c.d.s', 'lightning-fast', 'insurance-company'}\n",
      "len: 213\n",
      "Out-of-vocabulary words in test set: {'seven-yen', 'exxon-owned', '2,050-passenger', 'bread-and-butter', 'sewing-machine', '811.9', 'copper-rich', '143.80', '45-a-share', 'g.m.b', 'johnson-era', '566.54', 'garden-variety', 'headcount-control', 'guber\\\\/peters', 'per-share', '40-megabyte', 'waymar', '100-megabyte', '158,666', '2645.90', '319.75', '18-a-share', 'newspaper-printing', '-lcb-', 'reupke', '7\\\\/8', 'disputada', '361.8', 'unicorp', '120-a-share', 'house-senate', 'buy-outs', '387.8', 'launch-vehicle', '1206.26', 'junk-bond', 'weisfield', '19-month-old', 'hasbrouk', '3\\\\/8', 'prudential-bache', '1\\\\/2', 'stock-index', 'near-limit', 'anti-abortionists', 'heavy-truck', 'property\\\\/casualty', 'noncompetitively', 'yet-to-be-formed', 'year-earlier', 'bankruptcy-law', '-rcb-', 'larger-than-normal', '-lrb-', 'financial-services', 'ex-dividend', '142.84', '967,809', 'above-market', '237-seat', 'asset-valuation', 'cash-and-stock', 'txo', 'shareholder-rights', '377.60', '26,956', '734.9', '292.32', '5,699', '11,390,000', 'blue-chips', 'more-advanced', 'bronces', '131.01', '129.91', '87-store', 'louisiana-pacific', '126.15', '36-store', 'prior-year', 'cents-a-unit', '608,413', 'lentjes', 'computer-services', 'forest-products', '341.20', 'acquisition-minded', '43.875', '300-a-share', 'lobsenz', 'pro-iranian', '170,262', '5.2180', 'nofzinger', 'life-of-contract', '1\\\\/4', '220.45', 'cost-control', '0.0085', '3436.58', 'protein-1', 'early-retirement', 'waertsilae', '154,240,000', 'yoshihashi', '618.1', 'clean-air', 'high-rolling', 'information-services', '372.14', '188.84', '47.125', '83,206', '300-day', '1.916', '55-a-share', 'weapons-modernization', 'sino-u.s.', '3\\\\/4', 'inter-tel', '226,570,380', 'interleukin-3', 'ratners', '6,799', 'crookery', 'colorliner', '34.625', 'sept.30', 'arbitrage-related', 'hadson', 'mariotta', 'tete-a-tete', 'disputado', '494.50', '1\\\\/8', '11-month-old', 'diloreto', '630.9', '1.1650', 'passenger-car', '1.637', '38.875', 'profit-taking', 'manmade-fiber', 'corn-buying', 'derchin', '-rrb-', 'minimum-wage', 'several-year', '5\\\\/8', '434.4', 'intelogic', 'conn.based', 'fetal-tissue', 'constitutional-law', '50\\\\/50', '263.07', 'blood-cell', 'staff-reduction', 'nekoosa'}\n",
      "len: 161\n"
     ]
    }
   ],
   "source": [
    "# training OOV words\n",
    "\n",
    "existing_vocab_words = set(embedder.itos)  # Words in the vocabulary\n",
    "\n",
    "train_text = [word.lower() for word in df_train['text']]\n",
    "val_text = [word.lower() for word in df_val['text']]\n",
    "test_text = [word.lower() for word in df_test['text']]\n",
    "\n",
    "train_oov_words = set(train_text) - existing_vocab_words  # OOV words in training set\n",
    "val_oov_words = set(val_text) - existing_vocab_words  # OOV words in validation set\n",
    "test_oov_words = set(test_text) - existing_vocab_words  # OOV words in test set\n",
    "\n",
    "# print(existing_vocab_words)\n",
    "# \n",
    "print(\"Out-of-vocabulary words in training set:\", train_oov_words)\n",
    "print('len:', len(train_oov_words))\n",
    "print(\"Out-of-vocabulary words in validation set:\", val_oov_words)\n",
    "print('len:', len(val_oov_words))\n",
    "print(\"Out-of-vocabulary words in test set:\", test_oov_words)\n",
    "print('len:', len(test_oov_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 400000\n"
     ]
    }
   ],
   "source": [
    "print(len(embedder.itos),len(embedder.stoi.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-vocabulary words in validation set: {'2160.1', 'motor-home', 'ballantine\\\\/del', 'gates-warren', '6\\\\/2', 'investment-grade', '1937-40', 'heiwado', 'labor-backed', 'test-drive', 'less-than-brilliant', '8.575', '8.467', 'herald-american', 'sulfur-dioxide', 'money-market', 'multi-crystal', '14\\\\/32', 'million-a-year', 'egnuss', '238,000-circulation', 'parts-engineering', '2141.7', '352.9', 'stock-specialist', 'nylev', 'mega-stadium', '-lcb-', '30,537', 'non-callable', '7\\\\/8', 'n.v', 'anti-programmers', 'prevalance', 'ft-se', '95.09', 'yttrium-containing', '2003\\\\/2007', 'bottom-line', 'index-fund', 'house-senate', 'lynch-mob', 'buy-outs', 'wamre', 'bank-backed', 'four-foot-high', 'junk-bond', 'boogieman', '3\\\\/8', 'prudential-bache', 'veraldi', 'super-absorbent', 'floating-rate', 'mutchin', 'dydee', '1\\\\/2', 'sidak', 'stock-index', 'six-packs', 'arighi', '7.458', 'property\\\\/casualty', 'certin', 'express-buick', 'melt-textured', 'two-time-losers', 'one-newspaper', 'long-tenured', 'year-earlier', '877,663', 'single-lot', 'capital-markets', 'price-depressing', 'substance-abusing', 'crystal-lattice', '-rcb-', 'bridgestone\\\\/firestone', 'larger-than-normal', 'liquid-nitrogen', 'quantitive', '-lrb-', 'severable', 'financial-services', 'stock-picking', 'diceon', 'food-industry', '20-stock', 'potables', '372.9', 'executive-office', 'land-idling', '3,250,000', 'then-speaker', 'car-development', '13\\\\/16', 'cup-tote', '23,403', '35564.43', '71,309', 'scypher', 'circuit-board', '7.272', 'ednie', 'write-downs', 'freudtoy', 'crane-safety', 'flim-flammery', 'one-house', 'free-enterprise', '14.', 'incentive-backed', '29year', 'band-wagon', 'anku', 'much-larger', 'anti-program', 'credit-rating', '1928-33', 'midwesco', '3,040,000', '62%-owned', 'equity-purchase', '7.422', 'computer-driven', '11\\\\/16', 'seven-million-ton', '300-a-share', '300-113', 'truth-in-lending', 'lookee-loos', 'card-member', 'continuingly', '2163.2', '1\\\\/4', '7\\\\/16', '271-147', 'most-likely-successor', 'market-share', 'industry-supported', 'stock-price', 'disaster-assistance', 'anti-deficiency', 'triple-a-rated', 'minicrash', 'times-stock', 'clean-air', 'pension-fund', 'rey\\\\/fawcett', 'takeover-stock', 'housing-assistance', 'acid-rain', '47.125', 'year-ago', 'propagandizes', '9\\\\/32', 'corporate-wide', '3\\\\/4', 'car-care', 'enzor', '1738.1', '2\\\\/32', 'cleaner-burning', 'airline-related', '16\\\\/32', 'radio-station', '3648.82', 'anti-morning-sickness', 'fiber-end', '12\\\\/32', 'arbitraging', 'superdot', 'new-car', '13.625', 'foldability', 'pricings', 'citizen-sparked', 'double-c', 'index-arbitrage', 'middle-ground', 'stock-selection', '5.435', '22\\\\/32', 'triple-c', 'hart-scott-rodino', 'shokubai', 'price-support', '1\\\\/8', 'anti-miscarriage', 'intecknings', 'freshbake', '35500.64', 'program-trading', 'news-american', 'mutual-fund', 'tire-kickers', 'profit-taking', 'walbrecher', 'market-makers', 'high-polluting', 'breakey', '-rrb-', 'contingency-fee', 'electrical-safety', 'odd-year', '190-point', '5\\\\/8', 'phacoflex', 'heebie-jeebies', '2691.19', 'insider-trading', 'c.d.s', 'lightning-fast', 'insurance-company'}\n",
      "len: 213\n",
      "Out-of-vocabulary words in test set: {'seven-yen', 'exxon-owned', '2,050-passenger', 'bread-and-butter', 'sewing-machine', '811.9', 'copper-rich', '143.80', '45-a-share', 'g.m.b', 'johnson-era', '566.54', 'garden-variety', 'headcount-control', 'guber\\\\/peters', 'per-share', '40-megabyte', 'waymar', '100-megabyte', '158,666', '2645.90', '319.75', '18-a-share', 'newspaper-printing', '-lcb-', 'reupke', '7\\\\/8', 'disputada', '361.8', 'unicorp', '120-a-share', 'house-senate', 'buy-outs', '387.8', 'launch-vehicle', '1206.26', 'junk-bond', 'weisfield', '19-month-old', 'hasbrouk', '3\\\\/8', 'prudential-bache', '1\\\\/2', 'stock-index', 'near-limit', 'anti-abortionists', 'heavy-truck', 'property\\\\/casualty', 'noncompetitively', 'yet-to-be-formed', 'year-earlier', 'bankruptcy-law', '-rcb-', 'larger-than-normal', '-lrb-', 'financial-services', 'ex-dividend', '142.84', '967,809', 'above-market', '237-seat', 'asset-valuation', 'cash-and-stock', 'txo', 'shareholder-rights', '377.60', '26,956', '734.9', '292.32', '5,699', '11,390,000', 'blue-chips', 'more-advanced', 'bronces', '131.01', '129.91', '87-store', 'louisiana-pacific', '126.15', '36-store', 'prior-year', 'cents-a-unit', '608,413', 'lentjes', 'computer-services', 'forest-products', '341.20', 'acquisition-minded', '43.875', '300-a-share', 'lobsenz', 'pro-iranian', '170,262', '5.2180', 'nofzinger', 'life-of-contract', '1\\\\/4', '220.45', 'cost-control', '0.0085', '3436.58', 'protein-1', 'early-retirement', 'waertsilae', '154,240,000', 'yoshihashi', '618.1', 'clean-air', 'high-rolling', 'information-services', '372.14', '188.84', '47.125', '83,206', '300-day', '1.916', '55-a-share', 'weapons-modernization', 'sino-u.s.', '3\\\\/4', 'inter-tel', '226,570,380', 'interleukin-3', 'ratners', '6,799', 'crookery', 'colorliner', '34.625', 'sept.30', 'arbitrage-related', 'hadson', 'mariotta', 'tete-a-tete', 'disputado', '494.50', '1\\\\/8', '11-month-old', 'diloreto', '630.9', '1.1650', 'passenger-car', '1.637', '38.875', 'profit-taking', 'manmade-fiber', 'corn-buying', 'derchin', '-rrb-', 'minimum-wage', 'several-year', '5\\\\/8', '434.4', 'intelogic', 'conn.based', 'fetal-tissue', 'constitutional-law', '50\\\\/50', '263.07', 'blood-cell', 'staff-reduction', 'nekoosa'}\n",
      "len: 161\n"
     ]
    }
   ],
   "source": [
    "# OOV words\n",
    "\n",
    "existing_vocab_words = set(embedder.itos)  # Words in the vocabulary\n",
    "\n",
    "val_text = [word.lower() for word in df_val['text']]\n",
    "test_text = [word.lower() for word in df_test['text']]\n",
    "\n",
    "\n",
    "val_oov_words = set(val_text) - existing_vocab_words  # OOV words in validation set\n",
    "test_oov_words = set(test_text) - existing_vocab_words  # OOV words in test set\n",
    "\n",
    "# print(existing_vocab_words)\n",
    "# \n",
    "print(\"Out-of-vocabulary words in validation set:\", val_oov_words)\n",
    "print('len:', len(val_oov_words))\n",
    "print(\"Out-of-vocabulary words in test set:\", test_oov_words)\n",
    "print('len:', len(test_oov_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static embeddings\n",
    "static_embeddings_val = {}\n",
    "static_embeddings_test = {}\n",
    "\n",
    "# Assign static embeddings to OOV words in the validation set\n",
    "for word in val_oov_words:\n",
    "    if word in embedder.stoi:  #For each word assign a static embedding\n",
    "        static_embeddings_val[word] = embedder[word]\n",
    "\n",
    "# Assign static embeddings to OOV words in the test set\n",
    "for word in test_oov_words:\n",
    "    if word in embedder.stoi:\n",
    "        static_embeddings_test[word] = embedder[word]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_words_to_display = 3\n",
    "\n",
    "# print to check the validation set oov embeddings\n",
    "print(f\"Static embeddings for OOV words in the validation set:\")\n",
    "for i, (word, embedding) in enumerate(static_embeddings_val.items()):\n",
    "    if i < num_oov_words_to_display:\n",
    "        print(f\"Word: {word}, Embedding: {embedding}\")\n",
    "    else:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to check the test set oov embeddings\n",
    "print(f\"Static embeddings for OOV words in the test set:\")\n",
    "for i, (word, embedding) in enumerate(static_embeddings_test.items()):\n",
    "    if i < num_oov_words_to_display:\n",
    "        print(f\"Word: {word}, Embedding: {embedding}\")\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5: Training and Evaluation\n",
    "\n",
    "* Train **all** models on the train set.\n",
    "* Evaluate **all** models on the validation set.\n",
    "* Compute metrics on the validation set.\n",
    "* Pick **at least** three seeds for robust estimation.\n",
    "* Pick the **best** performing model according to the observed validation set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6: Error analysis\n",
    "\n",
    "* Compare the errors made on the validation and test sets.\n",
    "* Aggregate model errors into categories (if possible) \n",
    "* Comment about errors and propose possible solutions on how to address them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: REPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
