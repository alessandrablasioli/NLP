{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.077620500Z",
     "start_time": "2023-11-05T17:01:54.881071200Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install lightning\n",
    "# !pip install torchtext.data\n",
    "# !pip install torchtext\n",
    "# !pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.123165700Z",
     "start_time": "2023-11-05T17:02:00.080129600Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: remove unused dependencies\n",
    "\n",
    "# file management\n",
    "import sys\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# DL framework\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "# dataframe management\n",
    "import pandas as pd\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "\n",
    "# for readability\n",
    "from typing import Iterable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Corpus\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* **Download** the corpus.\n",
    "* **Encode** the corpus into a pandas.DataFrame object.\n",
    "* **Split** it in training, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.128673600Z",
     "start_time": "2023-11-05T17:02:00.095141800Z"
    }
   },
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_file:\n",
    "        zip_file.extractall(extract_path)\n",
    "\n",
    "    print(\"Extraction completed!\")\n",
    "\n",
    "    Path.unlink(download_path)\n",
    "    print(\"Deleted .zip dataset file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.129679600Z",
     "start_time": "2023-11-05T17:02:00.111162300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: c:\\Users\\merli\\OneDrive\\Documenti\\University\\Anno 5\\Natural Language Processing\\NLP\\A1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_zip_path.exists():\n",
    "    download_dataset(dataset_zip_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_zip_path, dataset_folder)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the corpus into a pandas.DataFrame object and split it into train, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains 200 documents.\n",
    "\n",
    "   * **Train**: Documents 1-100\n",
    "   * **Validation**: Documents 101-150\n",
    "   * **Test**: Documents 151-199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.235090600Z",
     "start_time": "2023-11-05T17:02:00.134678300Z"
    }
   },
   "outputs": [],
   "source": [
    "dataframe_rows = []  # list for DataFrame rows\n",
    "id = 0\n",
    "\n",
    "for i, file_path in enumerate(sorted(dataset_path.iterdir())):\n",
    "    if file_path.is_file(): # split corpus documents in the tree categories: train, validation, tests\n",
    "        if 1 <= i + 1 <= 100:\n",
    "            split = 'train'\n",
    "        elif 101 <= i + 1 <= 150:\n",
    "            split = 'validation'\n",
    "        else:\n",
    "            split = 'test'\n",
    "\n",
    "        with file_path.open(mode='r', encoding='utf-8') as text_file: # read corpus lines\n",
    "            lines = text_file.readlines()\n",
    "                \n",
    "        for line in lines:\n",
    "            fields = line.strip().split('\\t')\n",
    "            if len(fields) == 1:\n",
    "                id = id + 1\n",
    "            if len(fields) >= 2:\n",
    "                text = fields[0]  # store the first field as 'text'\n",
    "                POS = fields[1]   # store the second field as 'POS'\n",
    "                dataframe_row = {  #build DataFrame rows\n",
    "                    \"text\": text,\n",
    "                    \"POS\": POS,\n",
    "                    \"split\": split,\n",
    "                    \"id\": id\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row) #append rows\n",
    "# corpus DataFrame\n",
    "corpus_df = pd.DataFrame(dataframe_rows) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.250136600Z",
     "start_time": "2023-11-05T17:02:00.235090600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "      <th>split</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pierre</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vinken</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>CD</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>years</td>\n",
       "      <td>NNS</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>old</td>\n",
       "      <td>JJ</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>will</td>\n",
       "      <td>MD</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>join</td>\n",
       "      <td>VB</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     text  POS  split  id\n",
       "0  Pierre  NNP  train   0\n",
       "1  Vinken  NNP  train   0\n",
       "2       ,    ,  train   0\n",
       "3      61   CD  train   0\n",
       "4   years  NNS  train   0\n",
       "5     old   JJ  train   0\n",
       "6       ,    ,  train   0\n",
       "7    will   MD  train   0\n",
       "8    join   VB  train   0\n",
       "9     the   DT  train   0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.310630800Z",
     "start_time": "2023-11-05T17:02:00.251137400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train, test, validation split\n",
    "df_train = corpus_df[corpus_df['split'] == 'train'].drop(columns=['split'])\n",
    "df_test = corpus_df[corpus_df['split'] == 'test'].drop(columns=['split'])\n",
    "df_val = corpus_df[corpus_df['split'] == 'validation'].drop(columns=['split'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.318634400Z",
     "start_time": "2023-11-05T17:02:00.282289700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe structure:\n",
      "          text  POS  split    id\n",
      "0       Pierre  NNP  train     0\n",
      "1       Vinken  NNP  train     0\n",
      "2            ,    ,  train     0\n",
      "3           61   CD  train     0\n",
      "4        years  NNS  train     0\n",
      "...        ...  ...    ...   ...\n",
      "94079  quarter   NN   test  3715\n",
      "94080       of   IN   test  3715\n",
      "94081     next   JJ   test  3715\n",
      "94082     year   NN   test  3715\n",
      "94083        .    .   test  3715\n",
      "\n",
      "[94084 rows x 4 columns]\n",
      "\n",
      "Total rows 94084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe structure:\")\n",
    "print(corpus_df)\n",
    "print()\n",
    "\n",
    "print(\"Total rows %d\" % (len(corpus_df)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Text encoding\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed words unsing GloVe embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode text into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.319808300Z",
     "start_time": "2023-11-05T17:02:00.298801200Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe, build_vocab_from_iterator\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import loggers as pl_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.332320400Z",
     "start_time": "2023-11-05T17:02:00.313639300Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_embedding_model(embedding_dimension: int = 300):\n",
    "    emb_model = GloVe(name=\"6B\", dim=embedding_dimension)\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.531978600Z",
     "start_time": "2023-11-05T17:02:00.336322200Z"
    }
   },
   "outputs": [],
   "source": [
    "iterator = ([text] for text in corpus_df[\"POS\"].unique())\n",
    "vocab = build_vocab_from_iterator(iterator)\n",
    "\n",
    "\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, embedder):\n",
    "        min_id = dataframe['id'].min()\n",
    "        dataframe['id'] = dataframe['id'] - min_id\n",
    "        self.dataframe = dataframe.groupby(\"id\")\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.dataframe.get_group(idx)\n",
    "        text = sentence['text'].to_list()\n",
    "        POS = sentence['POS'].to_list()\n",
    "        \n",
    "        POS = torch.Tensor([vocab[token] for token in POS])\n",
    "        \n",
    "        POS_one_hot = torch.nn.functional.one_hot(POS.to(torch.int64), num_classes=len(vocab))\n",
    "        embedded_text = self.embedder.get_vecs_by_tokens(text)\n",
    "        \n",
    "        return embedded_text, POS_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.753077500Z",
     "start_time": "2023-11-05T17:02:00.533978200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definition of the dataset\n",
    "EMBEDDING_DIM = 50\n",
    "embedder = load_embedding_model(EMBEDDING_DIM)\n",
    "dataset_train = CorpusDataset(df_train, embedder)\n",
    "dataset_test = CorpusDataset(df_test, embedder)\n",
    "dataset_val = CorpusDataset(df_val, embedder)\n",
    "\n",
    "\n",
    "# TODO - test if it works in the LSTM training\n",
    "def my_collate(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels_padded = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "    \n",
    "    \n",
    "    sequences_padded = sequences_padded.type(torch.float)\n",
    "    labels_padded = labels_padded.type(torch.float)\n",
    "    \n",
    "    \n",
    "    return [sequences_padded, labels_padded]\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=3, collate_fn=my_collate)\n",
    "val_loader = DataLoader(dataset_val, batch_size=3, collate_fn=my_collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=3, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:00.820652100Z",
     "start_time": "2023-11-05T17:02:00.787586800Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTMModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiplied by 2 due to the bidirectionality\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out (batch_size, seq_length, hidden_size * 2)\n",
    "        out = self.fc(lstm_out)\n",
    "        # out (batch_size, seq_length, output_dim)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        # Change shape to be (batchsize, classes, sequence_len) to compute loss function\n",
    "        # TODO: Edo: should we normalize the loss by sequence_len?? Otherwise longer phrases influence the loss more\n",
    "        y = torch.movedim(y, 1, 2)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "\n",
    "        # TODO - check if it is a legit way to compute F1\n",
    "        y_class = torch.zeros(y.shape[0], y.shape[1]) \n",
    "        y_class = torch.argmax(y, dim=2)\n",
    "        y_class = torch.flatten(y)\n",
    "\n",
    "        y_hat_class = torch.zeros(y_hat.shape[0], y.shape[1]) \n",
    "        y_hat_class = torch.argmax(y_hat, dim=2)\n",
    "        y_hat_class = torch.flatten(y_hat)\n",
    "\n",
    "        f1 = F1Score(task=\"multiclass\", num_classes=self.output_dim)\n",
    "        f1_score = f1(y_hat_class, y_class)\n",
    "\n",
    "        self.log_dict({'train_f1': f1_score, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log_dict({'train_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = torch.movedim(y, 1, 2)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        self.log_dict({'val_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y = torch.movedim(y, 1, 2)\n",
    "        y_hat = torch.movedim(y_hat, 1, 2)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "\n",
    "        self.log_dict({'test_loss': loss, 'step': self.current_epoch}, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "version=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T17:02:01.330058800Z",
     "start_time": "2023-11-05T17:02:00.814635500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:630: Checkpoint directory logs\\lightning_logs\\version_2\\checkpoints exists and is not empty.\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | lstm | LSTM   | 17.9 K\n",
      "1 | fc   | Linear | 2.6 K \n",
      "--------------------------------\n",
      "20.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "20.5 K    Total params\n",
      "0.082     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:211: You called `self.log('step', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 622/622 [00:25<00:00, 23.99it/s, v_num=2, val_loss=0.405, step=9.000, train_f1=0.000, train_loss=0.337]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 622/622 [00:25<00:00, 23.97it/s, v_num=2, val_loss=0.405, step=9.000, train_f1=0.000, train_loss=0.337]\n"
     ]
    }
   ],
   "source": [
    "output_dim = len(df_train[\"POS\"].unique())\n",
    "input_dim = EMBEDDING_DIM\n",
    "hidden_dim = 28\n",
    "max_epochs = 10\n",
    "\n",
    "save_dir = \"logs\"\n",
    "load_model = False\n",
    "train_model = True \n",
    "model_ckpt = \"baseline.ckpt\"\n",
    "\n",
    "\n",
    "PATH = os.path.join(save_dir, \"lightning_logs\", \"version_\" + str(version), \"checkpoints\", model_ckpt)\n",
    "\n",
    "\n",
    "if load_model:\n",
    "    model = BiLSTMModel.load_from_checkpoint(PATH, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=1)\n",
    "else:\n",
    "    model = BiLSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=1)\n",
    "\n",
    "if train_model:\n",
    "    tb_logger = TensorBoardLogger(version=version, save_dir='logs')\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, logger=tb_logger, default_root_dir='./', log_every_n_steps=1)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    trainer.save_checkpoint(PATH)\n",
    "    version += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/202 [00:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LightningModule.log() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\merli\\OneDrive\\Documenti\\University\\Anno 5\\Natural Language Processing\\NLP\\A1\\A1_V1.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/merli/OneDrive/Documenti/University/Anno%205/Natural%20Language%20Processing/NLP/A1/A1_V1.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# test the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/merli/OneDrive/Documenti/University/Anno%205/Natural%20Language%20Processing/NLP/A1/A1_V1.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(model, dataloaders\u001b[39m=\u001b[39;49mtest_loader)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:755\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstatus \u001b[39m=\u001b[39m TrainerStatus\u001b[39m.\u001b[39mRUNNING\n\u001b[0;32m    754\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    756\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[0;32m    757\u001b[0m )\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:795\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    792\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    794\u001b[0m )\n\u001b[1;32m--> 795\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    796\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    797\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    992\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1029\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mzero_grad(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mzero_grad_kwargs)\n\u001b[0;32m   1028\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[1;32m-> 1029\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1031\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mis_last_batch \u001b[39m=\u001b[39m data_fetcher\u001b[39m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[0;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[39mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[39m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    311\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\nlp\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:416\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[0;32m    415\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 416\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32mc:\\Users\\merli\\OneDrive\\Documenti\\University\\Anno 5\\Natural Language Processing\\NLP\\A1\\A1_V1.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/merli/OneDrive/Documenti/University/Anno%205/Natural%20Language%20Processing/NLP/A1/A1_V1.ipynb#X33sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m y_hat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmovedim(y_hat, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/merli/OneDrive/Documenti/University/Anno%205/Natural%20Language%20Processing/NLP/A1/A1_V1.ipynb#X33sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(y_hat, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/merli/OneDrive/Documenti/University/Anno%205/Natural%20Language%20Processing/NLP/A1/A1_V1.ipynb#X33sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog({\u001b[39m'\u001b[39;49m\u001b[39mtest_loss\u001b[39;49m\u001b[39m'\u001b[39;49m: loss, \u001b[39m'\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_epoch}, on_epoch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, prog_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, logger\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/merli/OneDrive/Documenti/University/Anno%205/Natural%20Language%20Processing/NLP/A1/A1_V1.ipynb#X33sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[1;31mTypeError\u001b[0m: LightningModule.log() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 15512), started 0:33:54 ago. (Use '!kill 15512' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a4a345ded4ae064e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a4a345ded4ae064e\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class Model1(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super(Model1, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.lstm1 = nn.LSTM(input_size=input_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        \n",
    "       self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiplied by 2 due to bidirectionality\n",
    "        \n",
    "        # Additional LSTM layer\n",
    "        self.lstm2 = nn.LSTM(input_size=hidden_dim * 2, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out1, _ = self.lstm1(x)\n",
    "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
    "        out = self.fc(lstm_out2)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "\n",
    "        y_class = torch.argmax(y, dim=2).flatten()\n",
    "        y_hat_class = torch.argmax(y_hat, dim=2).flatten()\n",
    "        \n",
    "        f1 = F1Score(task=\"multiclass\", num_classes=self.output_dim).to(device)\n",
    "        f1_score = f1(y_hat_class, y_class)\n",
    "\n",
    "        self.log('train_f1', f1_score, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        \n",
    "        y_class = torch.argmax(y, dim=2).flatten()\n",
    "        y_hat_class = torch.argmax(y_hat, dim=2).flatten()\n",
    "        \n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.functional.cross_entropy(y_hat, y)\n",
    "        \n",
    "        y_class = torch.argmax(y, dim=2).flatten()\n",
    "        y_hat_class = torch.argmax(y_hat, dim=2).flatten()\n",
    "        \n",
    "        self.log('test_loss', loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model1 = Model1(input_dim=input_dim, hidden_dim=128, output_dim=output_dim, num_layers=1)\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model1, train_loader, val_loader)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "trainer.test(model1, dataloaders=test_loader)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
