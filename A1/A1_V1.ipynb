{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lightning in /home/elements72/.local/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/lib/python3/dist-packages (from lightning) (21.3)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (4.3.0)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/lib/python3/dist-packages (from lightning) (5.4.1)\n",
      "Requirement already satisfied: pytorch-lightning in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (1.9.4)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (0.11.4)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (4.64.1)\n",
      "Requirement already satisfied: torch<4.0,>=1.12.0 in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (2.1.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (1.23.3)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (0.8.0)\n",
      "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /home/elements72/.local/lib/python3.10/site-packages (from lightning) (2023.3.0)\n",
      "Requirement already satisfied: requests in /home/elements72/.local/lib/python3.10/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/elements72/.local/lib/python3.10/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.8.4)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.0.3)\n",
      "Requirement already satisfied: filelock in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (2.8.6)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (11.0.2.54)\n",
      "Requirement already satisfied: sympy in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (2.18.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/elements72/.local/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/elements72/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.12.0->lightning) (12.3.52)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/elements72/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/elements72/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/elements72/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/elements72/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/elements72/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/elements72/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/elements72/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.8.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/elements72/.local/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (1.26.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/elements72/.local/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext.data (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchtext.data\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lightning\n",
    "!pip install torchtext.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file management\n",
    "import sys\n",
    "import shutil\n",
    "import urllib\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# dataframe management\n",
    "import pandas as pd\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "\n",
    "# for readability\n",
    "from typing import Iterable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1: Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(download_path: Path, url: str):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=download_path, reporthook=t.update_to)\n",
    "\n",
    "        \n",
    "def download_dataset(download_path: Path, url: str):\n",
    "    print(\"Downloading dataset...\")\n",
    "    download_url(url=url, download_path=download_path)\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "def extract_dataset(download_path: Path, extract_path: Path):\n",
    "    print(\"Extracting dataset... (it may take a while...)\")\n",
    "    with zipfile.ZipFile(download_path, 'r') as zip_file:\n",
    "        zip_file.extractall(extract_path)\n",
    "\n",
    "    print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current work directory: /home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\"\n",
    "dataset_name = \"dependency_treebank\"\n",
    "\n",
    "print(f\"Current work directory: {Path.cwd()}\")\n",
    "\n",
    "dataset_folder = Path.cwd().joinpath(\"Datasets\")\n",
    "\n",
    "if not dataset_folder.exists():\n",
    "    dataset_folder.mkdir(parents=True)\n",
    "\n",
    "dataset_zip_path = dataset_folder.joinpath(\"dependency_treebank.zip\")\n",
    "dataset_path = dataset_folder.joinpath(dataset_name)\n",
    "\n",
    "if not dataset_zip_path.exists():\n",
    "    download_dataset(dataset_zip_path, url)\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    extract_dataset(dataset_zip_path, dataset_folder)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding the corpus into a pandas.DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus contains 200 documents.\n",
    "\n",
    "   * **Train**: Documents 1-100\n",
    "   * **Validation**: Documents 101-150\n",
    "   * **Test**: Documents 151-199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wsj_0033.dp...\n",
      "Processing wsj_0068.dp...\n",
      "Processing wsj_0076.dp...\n",
      "Processing wsj_0177.dp...\n",
      "Processing wsj_0082.dp...\n",
      "Processing wsj_0129.dp...\n",
      "Processing wsj_0078.dp...\n",
      "Processing wsj_0149.dp...\n",
      "Processing wsj_0057.dp...\n",
      "Processing wsj_0162.dp...\n",
      "Processing wsj_0174.dp...\n",
      "Processing wsj_0123.dp...\n",
      "Processing wsj_0020.dp...\n",
      "Processing wsj_0195.dp...\n",
      "Processing wsj_0161.dp...\n",
      "Processing wsj_0001.dp...\n",
      "Processing wsj_0193.dp...\n",
      "Processing wsj_0039.dp...\n",
      "Processing wsj_0036.dp...\n",
      "Processing wsj_0074.dp...\n",
      "Processing wsj_0059.dp...\n",
      "Processing wsj_0062.dp...\n",
      "Processing wsj_0048.dp...\n",
      "Processing wsj_0109.dp...\n",
      "Processing wsj_0188.dp...\n",
      "Processing wsj_0198.dp...\n",
      "Processing wsj_0029.dp...\n",
      "Processing wsj_0124.dp...\n",
      "Processing wsj_0079.dp...\n",
      "Processing wsj_0167.dp...\n",
      "Processing wsj_0120.dp...\n",
      "Processing wsj_0140.dp...\n",
      "Processing wsj_0131.dp...\n",
      "Processing wsj_0081.dp...\n",
      "Processing wsj_0031.dp...\n",
      "Processing wsj_0191.dp...\n",
      "Processing wsj_0019.dp...\n",
      "Processing wsj_0058.dp...\n",
      "Processing wsj_0137.dp...\n",
      "Processing wsj_0045.dp...\n",
      "Processing wsj_0025.dp...\n",
      "Processing wsj_0049.dp...\n",
      "Processing wsj_0107.dp...\n",
      "Processing wsj_0003.dp...\n",
      "Processing wsj_0026.dp...\n",
      "Processing wsj_0102.dp...\n",
      "Processing wsj_0054.dp...\n",
      "Processing wsj_0013.dp...\n",
      "Processing wsj_0061.dp...\n",
      "Processing wsj_0046.dp...\n",
      "Processing wsj_0173.dp...\n",
      "Processing wsj_0053.dp...\n",
      "Processing wsj_0066.dp...\n",
      "Processing wsj_0005.dp...\n",
      "Processing wsj_0189.dp...\n",
      "Processing wsj_0043.dp...\n",
      "Processing wsj_0080.dp...\n",
      "Processing wsj_0197.dp...\n",
      "Processing wsj_0095.dp...\n",
      "Processing wsj_0147.dp...\n",
      "Processing wsj_0034.dp...\n",
      "Processing wsj_0169.dp...\n",
      "Processing wsj_0159.dp...\n",
      "Processing wsj_0168.dp...\n",
      "Processing wsj_0097.dp...\n",
      "Processing wsj_0012.dp...\n",
      "Processing wsj_0181.dp...\n",
      "Processing wsj_0135.dp...\n",
      "Processing wsj_0187.dp...\n",
      "Processing wsj_0153.dp...\n",
      "Processing wsj_0136.dp...\n",
      "Processing wsj_0022.dp...\n",
      "Processing wsj_0116.dp...\n",
      "Processing wsj_0030.dp...\n",
      "Processing wsj_0139.dp...\n",
      "Processing wsj_0093.dp...\n",
      "Processing wsj_0134.dp...\n",
      "Processing wsj_0047.dp...\n",
      "Processing wsj_0178.dp...\n",
      "Processing wsj_0132.dp...\n",
      "Processing wsj_0041.dp...\n",
      "Processing wsj_0114.dp...\n",
      "Processing wsj_0096.dp...\n",
      "Processing wsj_0194.dp...\n",
      "Processing wsj_0125.dp...\n",
      "Processing wsj_0040.dp...\n",
      "Processing wsj_0044.dp...\n",
      "Processing wsj_0186.dp...\n",
      "Processing wsj_0032.dp...\n",
      "Processing wsj_0016.dp...\n",
      "Processing wsj_0091.dp...\n",
      "Processing wsj_0122.dp...\n",
      "Processing wsj_0111.dp...\n",
      "Processing wsj_0183.dp...\n",
      "Processing wsj_0151.dp...\n",
      "Processing wsj_0015.dp...\n",
      "Processing wsj_0185.dp...\n",
      "Processing wsj_0155.dp...\n",
      "Processing wsj_0160.dp...\n",
      "Processing wsj_0130.dp...\n",
      "Processing wsj_0067.dp...\n",
      "Processing wsj_0060.dp...\n",
      "Processing wsj_0011.dp...\n",
      "Processing wsj_0064.dp...\n",
      "Processing wsj_0196.dp...\n",
      "Processing wsj_0063.dp...\n",
      "Processing wsj_0070.dp...\n",
      "Processing wsj_0175.dp...\n",
      "Processing wsj_0127.dp...\n",
      "Processing wsj_0142.dp...\n",
      "Processing wsj_0128.dp...\n",
      "Processing wsj_0004.dp...\n",
      "Processing wsj_0146.dp...\n",
      "Processing wsj_0037.dp...\n",
      "Processing wsj_0110.dp...\n",
      "Processing wsj_0072.dp...\n",
      "Processing wsj_0171.dp...\n",
      "Processing wsj_0184.dp...\n",
      "Processing wsj_0027.dp...\n",
      "Processing wsj_0021.dp...\n",
      "Processing wsj_0166.dp...\n",
      "Processing wsj_0172.dp...\n",
      "Processing wsj_0104.dp...\n",
      "Processing wsj_0145.dp...\n",
      "Processing wsj_0158.dp...\n",
      "Processing wsj_0154.dp...\n",
      "Processing wsj_0103.dp...\n",
      "Processing wsj_0077.dp...\n",
      "Processing wsj_0017.dp...\n",
      "Processing wsj_0094.dp...\n",
      "Processing wsj_0098.dp...\n",
      "Processing wsj_0075.dp...\n",
      "Processing wsj_0141.dp...\n",
      "Processing wsj_0106.dp...\n",
      "Processing wsj_0069.dp...\n",
      "Processing wsj_0065.dp...\n",
      "Processing wsj_0099.dp...\n",
      "Processing wsj_0176.dp...\n",
      "Processing wsj_0115.dp...\n",
      "Processing wsj_0118.dp...\n",
      "Processing wsj_0101.dp...\n",
      "Processing wsj_0085.dp...\n",
      "Processing wsj_0105.dp...\n",
      "Processing wsj_0010.dp...\n",
      "Processing wsj_0009.dp...\n",
      "Processing wsj_0119.dp...\n",
      "Processing wsj_0023.dp...\n",
      "Processing wsj_0108.dp...\n",
      "Processing wsj_0192.dp...\n",
      "Processing wsj_0157.dp...\n",
      "Processing wsj_0126.dp...\n",
      "Processing wsj_0051.dp...\n",
      "Processing wsj_0182.dp...\n",
      "Processing wsj_0138.dp...\n",
      "Processing wsj_0199.dp...\n",
      "Processing wsj_0002.dp...\n",
      "Processing wsj_0179.dp...\n",
      "Processing wsj_0008.dp...\n",
      "Processing wsj_0014.dp...\n",
      "Processing wsj_0113.dp...\n",
      "Processing wsj_0073.dp...\n",
      "Processing wsj_0156.dp...\n",
      "Processing wsj_0035.dp...\n",
      "Processing wsj_0084.dp...\n",
      "Processing wsj_0190.dp...\n",
      "Processing wsj_0163.dp...\n",
      "Processing wsj_0024.dp...\n",
      "Processing wsj_0150.dp...\n",
      "Processing wsj_0133.dp...\n",
      "Processing wsj_0018.dp...\n",
      "Processing wsj_0121.dp...\n",
      "Processing wsj_0090.dp...\n",
      "Processing wsj_0148.dp...\n",
      "Processing wsj_0007.dp...\n",
      "Processing wsj_0028.dp...\n",
      "Processing wsj_0050.dp...\n",
      "Processing wsj_0144.dp...\n",
      "Processing wsj_0092.dp...\n",
      "Processing wsj_0117.dp...\n",
      "Processing wsj_0170.dp...\n",
      "Processing wsj_0164.dp...\n",
      "Processing wsj_0042.dp...\n",
      "Processing wsj_0038.dp...\n",
      "Processing wsj_0089.dp...\n",
      "Processing wsj_0143.dp...\n",
      "Processing wsj_0087.dp...\n",
      "Processing wsj_0088.dp...\n",
      "Processing wsj_0100.dp...\n",
      "Processing wsj_0083.dp...\n",
      "Processing wsj_0112.dp...\n",
      "Processing wsj_0006.dp...\n",
      "Processing wsj_0086.dp...\n",
      "Processing wsj_0055.dp...\n",
      "Processing wsj_0152.dp...\n",
      "Processing wsj_0056.dp...\n",
      "Processing wsj_0052.dp...\n",
      "Processing wsj_0165.dp...\n",
      "Processing wsj_0180.dp...\n",
      "Processing wsj_0071.dp...\n"
     ]
    }
   ],
   "source": [
    "dataframe_rows = []  # list for DataFrame rows\n",
    "for i, file_path in enumerate(dataset_path.iterdir()):\n",
    "    if file_path.is_file(): # split corpus documents in the tree categories: train, validation, tests\n",
    "        if 1 <= i + 1 <= 100:\n",
    "            split = 'train'\n",
    "        elif 101 <= i + 1 <= 150:\n",
    "            split = 'validation'\n",
    "        else:\n",
    "            split = 'test'\n",
    "\n",
    "        print(f\"Processing {file_path.name}...\")\n",
    "        with file_path.open(mode='r', encoding='utf-8') as text_file: # read corpus lines\n",
    "                lines = text_file.readlines()\n",
    "\n",
    "        if len(lines) > 0:\n",
    "        # split the first line based on tabs\n",
    "            fields = lines[0].strip().split('\\t')\n",
    "        if len(fields) >= 2:\n",
    "                text = fields[0]  # store the first field as 'text'\n",
    "                POS = fields[1]   # store the second field as 'POS'\n",
    "\n",
    "                dataframe_row = {  #build DataFrame rows\n",
    "                    \"text\": text,\n",
    "                    \"POS\": POS,\n",
    "                    \"split\": split\n",
    "                }\n",
    "\n",
    "                dataframe_rows.append(dataframe_row) #append rows\n",
    "\n",
    "# corpus DataFrame\n",
    "corpus_df = pd.DataFrame(dataframe_rows) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>POS</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOODY</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lancaster</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Komatsu</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Criticism</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lewis</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mitsui</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>New</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSI</td>\n",
       "      <td>NNP</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A</td>\n",
       "      <td>DT</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text  POS  split\n",
       "0      Japan  NNP  train\n",
       "1      GOODY  NNP  train\n",
       "2  Lancaster  NNP  train\n",
       "3    Komatsu  NNP  train\n",
       "4  Criticism  NNP  train\n",
       "5      Lewis  NNP  train\n",
       "6     Mitsui  NNP  train\n",
       "7        New  NNP  train\n",
       "8        LSI  NNP  train\n",
       "9          A   DT  train"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe structure:\n",
      "          text  POS  split\n",
      "0        Japan  NNP  train\n",
      "1        GOODY  NNP  train\n",
      "2    Lancaster  NNP  train\n",
      "3      Komatsu  NNP  train\n",
      "4    Criticism  NNP  train\n",
      "..         ...  ...    ...\n",
      "194          @   IN   test\n",
      "195     PAPERS  NNS   test\n",
      "196   Heritage  NNP   test\n",
      "197   Genetics  NNP   test\n",
      "198       When  WRB   test\n",
      "\n",
      "[199 rows x 3 columns]\n",
      "\n",
      "Total rows 199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe structure:\")\n",
    "print(corpus_df)\n",
    "print()\n",
    "\n",
    "print(\"Total rows %d\" % (len(corpus_df)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Text encoding\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Embed words using **GloVe embeddings**.\n",
    "* You are **free** to pick any embedding dimension.\n",
    "* [Optional] You are free to experiment with text pre-processing: **make sure you do not delete any token!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode text into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "def load_embedding_model(embedding_dimension: int = 50):\n",
    "    emb_model = GloVe(name=\"6B\", dim=300)\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip:  14%|█▍        | 124M/862M [10:41<1:03:26, 194kB/s]    \n"
     ]
    },
    {
     "ename": "ContentTooShortError",
     "evalue": "<urlopen error retrieval incomplete: got only 124305149 out of 862182613 bytes>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContentTooShortError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1/A1_V1.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1/A1_V1.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m\"\u001b[39m\u001b[39mbasic_english\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m## We'll use tokenizer available from PyTorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1/A1_V1.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m load_embedding_model()\n",
      "\u001b[1;32m/home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1/A1_V1.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1/A1_V1.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_embedding_model\u001b[39m(embedding_dimension: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1/A1_V1.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     emb_model \u001b[39m=\u001b[39m GloVe(name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m6B\u001b[39;49m\u001b[39m\"\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/elements72/unibo/Vanno/NLP/Assignments/NLP/A1/A1_V1.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m emb_model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/vocab/vectors.py:223\u001b[0m, in \u001b[0;36mGloVe.__init__\u001b[0;34m(self, name, dim, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl[name]\n\u001b[1;32m    222\u001b[0m name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mglove.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39md.txt\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name, \u001b[39mstr\u001b[39m(dim))\n\u001b[0;32m--> 223\u001b[0m \u001b[39msuper\u001b[39;49m(GloVe, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(name, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/vocab/vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munk_init \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mzero_ \u001b[39mif\u001b[39;00m unk_init \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m unk_init\n\u001b[0;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache(name, cache, url\u001b[39m=\u001b[39;49murl, max_vectors\u001b[39m=\u001b[39;49mmax_vectors)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchtext/vocab/vectors.py:98\u001b[0m, in \u001b[0;36mVectors.cache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m, unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, miniters\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, desc\u001b[39m=\u001b[39mdest) \u001b[39mas\u001b[39;00m t:\n\u001b[1;32m     97\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m         urlretrieve(url, dest, reporthook\u001b[39m=\u001b[39;49mreporthook(t))\n\u001b[1;32m     99\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# remove the partial zip file\u001b[39;00m\n\u001b[1;32m    100\u001b[0m         os\u001b[39m.\u001b[39mremove(dest)\n",
      "File \u001b[0;32m/usr/lib/python3.10/urllib/request.py:280\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 reporthook(blocknum, bs, size)\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m read \u001b[39m<\u001b[39m size:\n\u001b[0;32m--> 280\u001b[0m     \u001b[39mraise\u001b[39;00m ContentTooShortError(\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mretrieval incomplete: got only \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m out of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m bytes\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m%\u001b[39m (read, size), result)\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mContentTooShortError\u001b[0m: <urlopen error retrieval incomplete: got only 124305149 out of 862182613 bytes>"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\") ## We'll use tokenizer available from PyTorch\n",
    "\n",
    "load_embedding_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
